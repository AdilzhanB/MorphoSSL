{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac5ee8da",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-30T14:34:35.564893Z",
     "iopub.status.busy": "2025-12-30T14:34:35.564686Z",
     "iopub.status.idle": "2025-12-30T14:34:35.571082Z",
     "shell.execute_reply": "2025-12-30T14:34:35.570643Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.011356,
     "end_time": "2025-12-30T14:34:35.572042",
     "exception": false,
     "start_time": "2025-12-30T14:34:35.560686",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing vision_transformer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile vision_transformer.py\n",
    "from __future__ import annotations\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Literal, Optional\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class ViTConfig:\n",
    "    img_size: int = 224\n",
    "    patch_size: int = 16\n",
    "    in_chans: int = 3\n",
    "    embed_dim: int = 768\n",
    "    depth: int = 12\n",
    "    num_heads: int = 12\n",
    "    mlp_ratio: float = 4.0\n",
    "    qkv_bias: bool = True\n",
    "    drop_rate: float = 0.0\n",
    "    attn_drop_rate: float = 0.0\n",
    "    drop_path_rate: float = 0.0\n",
    "\n",
    "def make_vit_config(size: Literal[\"base\", \"large\"] = \"base\", *, img_size: int = 224, patch_size: int = 16, in_chans: int = 3) -> ViTConfig:\n",
    "    if size == \"base\":\n",
    "        return ViTConfig(img_size=img_size, patch_size=patch_size, in_chans=in_chans, embed_dim=768, depth=12, num_heads=12)\n",
    "    if size == \"large\":\n",
    "        return ViTConfig(img_size=img_size, patch_size=patch_size, in_chans=in_chans, embed_dim=1024, depth=24, num_heads=16)\n",
    "    raise ValueError(f\"Unknown ViT size: {size}\")\n",
    "\n",
    "def build_vit_encoder(cfg: ViTConfig) -> nn.Module:\n",
    "    try:\n",
    "        from timm.models.vision_transformer import VisionTransformer\n",
    "    except Exception as exc:\n",
    "        raise RuntimeError(\n",
    "            \"timm is required for this project. Install with: pip install timm\"\n",
    "        ) from exc\n",
    "\n",
    "    model = VisionTransformer(\n",
    "        img_size=cfg.img_size,\n",
    "        patch_size=cfg.patch_size,\n",
    "        in_chans=cfg.in_chans,\n",
    "        num_classes=0,\n",
    "        embed_dim=cfg.embed_dim,\n",
    "        depth=cfg.depth,\n",
    "        num_heads=cfg.num_heads,\n",
    "        mlp_ratio=cfg.mlp_ratio,\n",
    "        qkv_bias=cfg.qkv_bias,\n",
    "        drop_rate=cfg.drop_rate,\n",
    "        attn_drop_rate=cfg.attn_drop_rate,\n",
    "        drop_path_rate=cfg.drop_path_rate,\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def vit_num_patches(vit: nn.Module) -> int:\n",
    "    if hasattr(vit, \"patch_embed\") and hasattr(vit.patch_embed, \"num_patches\"):\n",
    "        return int(vit.patch_embed.num_patches)\n",
    "    raise ValueError(\"Could not infer num_patches from ViT instance\")\n",
    "\n",
    "def vit_embed_dim(vit: nn.Module) -> int:\n",
    "    if hasattr(vit, \"embed_dim\"):\n",
    "        return int(vit.embed_dim)\n",
    "    raise ValueError(\"Could not infer embed_dim from ViT instance\")\n",
    "\n",
    "@torch.no_grad()\n",
    "def get_2d_sincos_pos_embed(embed_dim: int, grid_size: int, *, cls_token: bool = False, device: Optional[torch.device] = None) -> torch.Tensor:\n",
    "    if embed_dim % 4 != 0:\n",
    "        raise ValueError(\"embed_dim must be divisible by 4 for 2D sincos embedding\")\n",
    "\n",
    "    device = device or torch.device(\"cpu\")\n",
    "    grid_h = torch.arange(grid_size, device=device, dtype=torch.float32)\n",
    "    grid_w = torch.arange(grid_size, device=device, dtype=torch.float32)\n",
    "    grid = torch.stack(torch.meshgrid(grid_w, grid_h, indexing=\"xy\"), dim=0)\n",
    "    grid = grid.reshape(2, 1, grid_size, grid_size)\n",
    "\n",
    "    emb_h = _get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[0])\n",
    "    emb_w = _get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[1])\n",
    "    pos_embed = torch.cat([emb_h, emb_w], dim=1)\n",
    "\n",
    "    if cls_token:\n",
    "        cls = torch.zeros([1, embed_dim], device=device, dtype=pos_embed.dtype)\n",
    "        pos_embed = torch.cat([cls, pos_embed], dim=0)\n",
    "    return pos_embed\n",
    "\n",
    "def _get_1d_sincos_pos_embed_from_grid(embed_dim: int, pos: torch.Tensor) -> torch.Tensor:\n",
    "    pos = pos.reshape(-1)\n",
    "    omega = torch.arange(embed_dim // 2, device=pos.device, dtype=torch.float32)\n",
    "    omega = 1.0 / (10000 ** (omega / (embed_dim / 2)))\n",
    "    out = torch.einsum(\"m,d->md\", pos, omega)\n",
    "    emb_sin = torch.sin(out)\n",
    "    emb_cos = torch.cos(out)\n",
    "    return torch.cat([emb_sin, emb_cos], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a9ad35",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-30T14:34:35.576865Z",
     "iopub.status.busy": "2025-12-30T14:34:35.576713Z",
     "iopub.status.idle": "2025-12-30T14:34:35.581359Z",
     "shell.execute_reply": "2025-12-30T14:34:35.580981Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.007945,
     "end_time": "2025-12-30T14:34:35.582162",
     "exception": false,
     "start_time": "2025-12-30T14:34:35.574217",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing model_mae.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile model_mae.py\n",
    "from __future__ import annotations\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from vision_transformer import get_2d_sincos_pos_embed\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class MAEConfig:\n",
    "    img_size: int = 224\n",
    "    patch_size: int = 16\n",
    "    in_chans: int = 3\n",
    "\n",
    "    embed_dim: int = 768\n",
    "    depth: int = 12\n",
    "    num_heads: int = 12\n",
    "    mlp_ratio: float = 4.0\n",
    "    qkv_bias: bool = True\n",
    "\n",
    "    decoder_embed_dim: int = 512\n",
    "    decoder_depth: int = 8\n",
    "    decoder_num_heads: int = 16\n",
    "    decoder_mlp_ratio: float = 4.0\n",
    "\n",
    "    mask_ratio: float = 0.75\n",
    "    norm_pix_loss: bool = False\n",
    "\n",
    "\n",
    "class MaskedAutoencoderViT(nn.Module):\n",
    "    def __init__(self, cfg: MAEConfig):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "\n",
    "        try:\n",
    "            from timm.models.vision_transformer import Block, PatchEmbed\n",
    "        except Exception as exc:  \n",
    "            raise RuntimeError(\"timm is required. Install with: pip install timm\") from exc\n",
    "\n",
    "        self.patch_embed = PatchEmbed(\n",
    "            img_size=cfg.img_size,\n",
    "            patch_size=cfg.patch_size,\n",
    "            in_chans=cfg.in_chans,\n",
    "            embed_dim=cfg.embed_dim,\n",
    "        )\n",
    "        num_patches = self.patch_embed.num_patches\n",
    "        self.num_patches = int(num_patches)\n",
    "\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, cfg.embed_dim))\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, 1 + num_patches, cfg.embed_dim), requires_grad=False)\n",
    "\n",
    "        self.blocks = nn.ModuleList(\n",
    "            [\n",
    "                Block(\n",
    "                    dim=cfg.embed_dim,\n",
    "                    num_heads=cfg.num_heads,\n",
    "                    mlp_ratio=cfg.mlp_ratio,\n",
    "                    qkv_bias=cfg.qkv_bias,\n",
    "                    norm_layer=nn.LayerNorm,\n",
    "                )\n",
    "                for _ in range(cfg.depth)\n",
    "            ]\n",
    "        )\n",
    "        self.norm = nn.LayerNorm(cfg.embed_dim)\n",
    "\n",
    "        self.decoder_embed = nn.Linear(cfg.embed_dim, cfg.decoder_embed_dim, bias=True)\n",
    "        self.mask_token = nn.Parameter(torch.zeros(1, 1, cfg.decoder_embed_dim))\n",
    "        self.decoder_pos_embed = nn.Parameter(\n",
    "            torch.zeros(1, 1 + num_patches, cfg.decoder_embed_dim), requires_grad=False\n",
    "        )\n",
    "\n",
    "        self.decoder_blocks = nn.ModuleList(\n",
    "            [\n",
    "                Block(\n",
    "                    dim=cfg.decoder_embed_dim,\n",
    "                    num_heads=cfg.decoder_num_heads,\n",
    "                    mlp_ratio=cfg.decoder_mlp_ratio,\n",
    "                    qkv_bias=cfg.qkv_bias,\n",
    "                    norm_layer=nn.LayerNorm,\n",
    "                )\n",
    "                for _ in range(cfg.decoder_depth)\n",
    "            ]\n",
    "        )\n",
    "        self.decoder_norm = nn.LayerNorm(cfg.decoder_embed_dim)\n",
    "        self.decoder_pred = nn.Linear(cfg.decoder_embed_dim, cfg.patch_size * cfg.patch_size * cfg.in_chans, bias=True)\n",
    "\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self) -> None:\n",
    "        grid_size = int((self.cfg.img_size // self.cfg.patch_size))\n",
    "        pos_embed = get_2d_sincos_pos_embed(self.cfg.embed_dim, grid_size, cls_token=True)\n",
    "        self.pos_embed.data.copy_(pos_embed.unsqueeze(0))\n",
    "\n",
    "        dec_pos_embed = get_2d_sincos_pos_embed(self.cfg.decoder_embed_dim, grid_size, cls_token=True)\n",
    "        self.decoder_pos_embed.data.copy_(dec_pos_embed.unsqueeze(0))\n",
    "\n",
    "        nn.init.normal_(self.cls_token, std=0.02)\n",
    "        nn.init.normal_(self.mask_token, std=0.02)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "            elif isinstance(m, nn.LayerNorm):\n",
    "                nn.init.zeros_(m.bias)\n",
    "                nn.init.ones_(m.weight)\n",
    "\n",
    "    def patchify(self, imgs: torch.Tensor) -> torch.Tensor:\n",
    "        p = self.cfg.patch_size\n",
    "        n, c, h, w = imgs.shape\n",
    "        if h != w or h % p != 0:\n",
    "            raise ValueError(f\"Expected square images divisible by patch_size={p}. Got {h}x{w}.\")\n",
    "        gs = h // p\n",
    "        x = imgs.reshape(n, c, gs, p, gs, p)\n",
    "        x = x.permute(0, 2, 4, 3, 5, 1).contiguous()  \n",
    "        x = x.reshape(n, gs * gs, p * p * c)\n",
    "        return x\n",
    "\n",
    "    def unpatchify(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        p = self.cfg.patch_size\n",
    "        n, l, ppcc = x.shape\n",
    "        c = self.cfg.in_chans\n",
    "        if ppcc != p * p * c:\n",
    "            raise ValueError(\"Patch dimension mismatch\")\n",
    "        gs = int(l ** 0.5)\n",
    "        if gs * gs != l:\n",
    "            raise ValueError(\"L must be a square number\")\n",
    "        x = x.reshape(n, gs, gs, p, p, c)\n",
    "        x = x.permute(0, 5, 1, 3, 2, 4).contiguous()\n",
    "        imgs = x.reshape(n, c, gs * p, gs * p)\n",
    "        return imgs\n",
    "\n",
    "    def random_masking(self, x: torch.Tensor, mask_ratio: float) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        n, l, d = x.shape\n",
    "        len_keep = int(l * (1 - mask_ratio))\n",
    "\n",
    "        noise = torch.rand(n, l, device=x.device)\n",
    "        ids_shuffle = torch.argsort(noise, dim=1)\n",
    "        ids_restore = torch.argsort(ids_shuffle, dim=1)\n",
    "\n",
    "        ids_keep = ids_shuffle[:, :len_keep]\n",
    "        x_masked = torch.gather(x, dim=1, index=ids_keep.unsqueeze(-1).repeat(1, 1, d))\n",
    "\n",
    "        mask = torch.ones([n, l], device=x.device)\n",
    "        mask[:, :len_keep] = 0\n",
    "        mask = torch.gather(mask, dim=1, index=ids_restore)\n",
    "\n",
    "        return x_masked, mask, ids_restore\n",
    "\n",
    "    def forward_encoder(self, imgs: torch.Tensor, mask_ratio: float) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        x = self.patch_embed(imgs)\n",
    "        x = x + self.pos_embed[:, 1:, :]\n",
    "\n",
    "        x, mask, ids_restore = self.random_masking(x, mask_ratio)\n",
    "\n",
    "        cls_token = self.cls_token + self.pos_embed[:, :1, :]\n",
    "        cls_tokens = cls_token.expand(x.shape[0], -1, -1)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x)\n",
    "        x = self.norm(x)\n",
    "        return x, mask, ids_restore\n",
    "\n",
    "    def forward_decoder(self, x: torch.Tensor, ids_restore: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.decoder_embed(x)\n",
    "\n",
    "        x_vis = x[:, 1:, :]\n",
    "        n, l_vis, d = x_vis.shape\n",
    "        l = ids_restore.shape[1]\n",
    "\n",
    "        mask_tokens = self.mask_token.repeat(n, l - l_vis, 1)\n",
    "        x_ = torch.cat([x_vis, mask_tokens], dim=1)\n",
    "        x_ = torch.gather(x_, dim=1, index=ids_restore.unsqueeze(-1).repeat(1, 1, d))\n",
    "\n",
    "        x = torch.cat([x[:, :1, :], x_], dim=1)\n",
    "        x = x + self.decoder_pos_embed\n",
    "\n",
    "        for blk in self.decoder_blocks:\n",
    "            x = blk(x)\n",
    "        x = self.decoder_norm(x)\n",
    "\n",
    "        x = self.decoder_pred(x)\n",
    "        x = x[:, 1:, :]\n",
    "        return x\n",
    "\n",
    "    def forward_loss(self, imgs: torch.Tensor, pred: torch.Tensor, mask: torch.Tensor) -> torch.Tensor:\n",
    "        target = self.patchify(imgs)\n",
    "\n",
    "        if self.cfg.norm_pix_loss:\n",
    "            mean = target.mean(dim=-1, keepdim=True)\n",
    "            var = target.var(dim=-1, keepdim=True)\n",
    "            target = (target - mean) / (var + 1.0e-6) ** 0.5\n",
    "\n",
    "        loss = (pred - target) ** 2\n",
    "        loss = loss.mean(dim=-1) \n",
    "\n",
    "        loss = (loss * mask).sum() / mask.sum().clamp_min(1.0)\n",
    "        return loss\n",
    "\n",
    "    def forward(self, imgs: torch.Tensor, *, mask_ratio: Optional[float] = None) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        mask_ratio = self.cfg.mask_ratio if mask_ratio is None else float(mask_ratio)\n",
    "        latent, mask, ids_restore = self.forward_encoder(imgs, mask_ratio)\n",
    "        pred = self.forward_decoder(latent, ids_restore) \n",
    "        loss = self.forward_loss(imgs, pred, mask)\n",
    "        return loss, pred, mask\n",
    "\n",
    "\n",
    "def build_mae_vit_base(\n",
    "    *,\n",
    "    img_size: int = 224,\n",
    "    patch_size: int = 16,\n",
    "    in_chans: int = 3,\n",
    "    mask_ratio: float = 0.75,\n",
    "    norm_pix_loss: bool = False,\n",
    ") -> MaskedAutoencoderViT:\n",
    "    cfg = MAEConfig(\n",
    "        img_size=img_size,\n",
    "        patch_size=patch_size,\n",
    "        in_chans=in_chans,\n",
    "        embed_dim=768,\n",
    "        depth=12,\n",
    "        num_heads=12,\n",
    "        decoder_embed_dim=512,\n",
    "        decoder_depth=8,\n",
    "        decoder_num_heads=16,\n",
    "        mask_ratio=mask_ratio,\n",
    "        norm_pix_loss=norm_pix_loss,\n",
    "    )\n",
    "    return MaskedAutoencoderViT(cfg)\n",
    "\n",
    "\n",
    "def build_mae_vit_large(\n",
    "    *,\n",
    "    img_size: int = 224,\n",
    "    patch_size: int = 16,\n",
    "    in_chans: int = 3,\n",
    "    mask_ratio: float = 0.75,\n",
    "    norm_pix_loss: bool = False,\n",
    ") -> MaskedAutoencoderViT:\n",
    "    cfg = MAEConfig(\n",
    "        img_size=img_size,\n",
    "        patch_size=patch_size,\n",
    "        in_chans=in_chans,\n",
    "        embed_dim=1024,\n",
    "        depth=24,\n",
    "        num_heads=16,\n",
    "        decoder_embed_dim=512,\n",
    "        decoder_depth=8,\n",
    "        decoder_num_heads=16,\n",
    "        mask_ratio=mask_ratio,\n",
    "        norm_pix_loss=norm_pix_loss,\n",
    "    )\n",
    "    return MaskedAutoencoderViT(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ee8c60",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-30T14:34:35.586885Z",
     "iopub.status.busy": "2025-12-30T14:34:35.586742Z",
     "iopub.status.idle": "2025-12-30T14:34:35.590105Z",
     "shell.execute_reply": "2025-12-30T14:34:35.589733Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.006686,
     "end_time": "2025-12-30T14:34:35.590929",
     "exception": false,
     "start_time": "2025-12-30T14:34:35.584243",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing eval_module.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile eval_module.py\n",
    "from __future__ import annotations\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Optional, Tuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class EvalConfig:\n",
    "    num_classes: int\n",
    "    lr: float = 1e-4\n",
    "    encoder_lr: float | None = None\n",
    "    weight_decay: float = 0.01\n",
    "    freeze_encoder: bool = False\n",
    "\n",
    "\n",
    "class ViTClassifier(pl.LightningModule):\n",
    "    def __init__(self, encoder: nn.Module, cfg: EvalConfig):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters({**cfg.__dict__})\n",
    "        self.cfg = cfg\n",
    "        self.encoder = encoder\n",
    "        embed_dim = getattr(encoder, \"embed_dim\", None)\n",
    "        if embed_dim is None:\n",
    "            raise ValueError(\"Could not infer encoder embed_dim\")\n",
    "\n",
    "        self.head = nn.Linear(int(embed_dim), int(cfg.num_classes))\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        for p in self.encoder.parameters():\n",
    "            p.requires_grad = not bool(cfg.freeze_encoder)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        feats = self._forward_features(x)\n",
    "        return self.head(feats)\n",
    "\n",
    "    def _forward_features(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        if hasattr(self.encoder, \"forward_features\"):\n",
    "            feats = self.encoder.forward_features(x)\n",
    "        else:\n",
    "            feats = self.encoder(x)\n",
    "\n",
    "        if feats.dim() == 3:\n",
    "            feats = feats[:, 0]\n",
    "        return feats\n",
    "\n",
    "    def training_step(self, batch: Any, batch_idx: int) -> torch.Tensor:\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = self.criterion(logits, y)\n",
    "        acc = (logits.argmax(dim=1) == y).float().mean()\n",
    "        self.log(\"train/loss\", loss, prog_bar=True, on_step=True, on_epoch=True)\n",
    "        self.log(\"train/acc\", acc, prog_bar=True, on_step=True, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch: Any, batch_idx: int) -> None:\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = self.criterion(logits, y)\n",
    "        acc = (logits.argmax(dim=1) == y).float().mean()\n",
    "        self.log(\"val/loss\", loss, prog_bar=True, on_step=False, on_epoch=True)\n",
    "        self.log(\"val/acc\", acc, prog_bar=True, on_step=False, on_epoch=True)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        if self.cfg.freeze_encoder:\n",
    "            params = [{\"params\": self.head.parameters(), \"lr\": self.cfg.lr}]\n",
    "        else:\n",
    "            enc_lr = self.cfg.encoder_lr if self.cfg.encoder_lr is not None else self.cfg.lr * 0.1\n",
    "            params = [\n",
    "                {\"params\": self.encoder.parameters(), \"lr\": enc_lr},\n",
    "                {\"params\": self.head.parameters(), \"lr\": self.cfg.lr},\n",
    "            ]\n",
    "        opt = torch.optim.AdamW(params, lr=self.cfg.lr, weight_decay=self.cfg.weight_decay)\n",
    "        return opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6fc74de8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-30T14:34:35.595623Z",
     "iopub.status.busy": "2025-12-30T14:34:35.595489Z",
     "iopub.status.idle": "2025-12-30T14:34:35.598730Z",
     "shell.execute_reply": "2025-12-30T14:34:35.598353Z"
    },
    "papermill": {
     "duration": 0.006634,
     "end_time": "2025-12-30T14:34:35.599587",
     "exception": false,
     "start_time": "2025-12-30T14:34:35.592953",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing resnet_eval_module.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile resnet_eval_module.py\n",
    "from __future__ import annotations\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Any\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ResNetConfig:\n",
    "    arch: str\n",
    "    pretrained: bool\n",
    "    num_classes: int\n",
    "    lr: float = 1e-4\n",
    "    encoder_lr: float | None = None\n",
    "    weight_decay: float = 0.01\n",
    "    freeze_encoder: bool = False\n",
    "\n",
    "\n",
    "class ResNetClassifier(pl.LightningModule):\n",
    "    def __init__(self, cfg: ResNetConfig):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters({**cfg.__dict__})\n",
    "        self.cfg = cfg\n",
    "\n",
    "        from torchvision.models import ResNet18_Weights, ResNet50_Weights, resnet18, resnet50\n",
    "\n",
    "        if cfg.arch == \"resnet18\":\n",
    "            weights = ResNet18_Weights.DEFAULT if cfg.pretrained else None\n",
    "            base = resnet18(weights=weights)\n",
    "        elif cfg.arch == \"resnet50\":\n",
    "            weights = ResNet50_Weights.DEFAULT if cfg.pretrained else None\n",
    "            base = resnet50(weights=weights)\n",
    "        else:\n",
    "            raise ValueError(\"arch must be resnet18 or resnet50\")\n",
    "\n",
    "        feat_dim = int(base.fc.in_features)\n",
    "        base.fc = nn.Identity()\n",
    "        self.encoder = base\n",
    "\n",
    "        for p in self.encoder.parameters():\n",
    "            p.requires_grad = not bool(cfg.freeze_encoder)\n",
    "\n",
    "        self.head = nn.Linear(feat_dim, int(cfg.num_classes))\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        feats = self.encoder(x)\n",
    "        return self.head(feats)\n",
    "\n",
    "    def training_step(self, batch: Any, batch_idx: int) -> torch.Tensor:\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = self.criterion(logits, y)\n",
    "        acc = (logits.argmax(dim=1) == y).float().mean()\n",
    "        self.log(\"train/loss\", loss, prog_bar=True, on_step=True, on_epoch=True)\n",
    "        self.log(\"train/acc\", acc, prog_bar=True, on_step=True, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch: Any, batch_idx: int) -> None:\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = self.criterion(logits, y)\n",
    "        acc = (logits.argmax(dim=1) == y).float().mean()\n",
    "        self.log(\"val/loss\", loss, prog_bar=True, on_step=False, on_epoch=True)\n",
    "        self.log(\"val/acc\", acc, prog_bar=True, on_step=False, on_epoch=True)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        if self.cfg.freeze_encoder:\n",
    "            params = [{\"params\": self.head.parameters(), \"lr\": self.cfg.lr}]\n",
    "        else:\n",
    "            enc_lr = self.cfg.encoder_lr if self.cfg.encoder_lr is not None else self.cfg.lr * 0.1\n",
    "            params = [\n",
    "                {\"params\": self.encoder.parameters(), \"lr\": enc_lr},\n",
    "                {\"params\": self.head.parameters(), \"lr\": self.cfg.lr},\n",
    "            ]\n",
    "        return torch.optim.AdamW(params, lr=self.cfg.lr, weight_decay=self.cfg.weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3e4e14",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-30T14:34:35.604631Z",
     "iopub.status.busy": "2025-12-30T14:34:35.604484Z",
     "iopub.status.idle": "2025-12-30T14:34:35.608374Z",
     "shell.execute_reply": "2025-12-30T14:34:35.607960Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.007539,
     "end_time": "2025-12-30T14:34:35.609240",
     "exception": false,
     "start_time": "2025-12-30T14:34:35.601701",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing pretrain_module.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile pretrain_module.py\n",
    "from __future__ import annotations\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, Optional\n",
    "\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from model_mae import MaskedAutoencoderViT, build_mae_vit_base, build_mae_vit_large\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class PretrainConfig:\n",
    "    backbone: str = \"vit_base\"  \n",
    "    img_size: int = 224\n",
    "    patch_size: int = 16\n",
    "    in_chans: int = 3\n",
    "\n",
    "    mask_ratio: float = 0.75\n",
    "    norm_pix_loss: bool = False\n",
    "\n",
    "    lr: float = 1.5e-4\n",
    "    weight_decay: float = 0.05\n",
    "\n",
    "\n",
    "class MAEPretrainModule(pl.LightningModule):\n",
    "    def __init__(self, cfg: PretrainConfig):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters(cfg.__dict__)\n",
    "        self.cfg = cfg\n",
    "\n",
    "        if cfg.backbone == \"vit_base\":\n",
    "            self.model: MaskedAutoencoderViT = build_mae_vit_base(\n",
    "                img_size=cfg.img_size,\n",
    "                patch_size=cfg.patch_size,\n",
    "                in_chans=cfg.in_chans,\n",
    "                mask_ratio=cfg.mask_ratio,\n",
    "                norm_pix_loss=cfg.norm_pix_loss,\n",
    "            )\n",
    "        elif cfg.backbone == \"vit_large\":\n",
    "            self.model = build_mae_vit_large(\n",
    "                img_size=cfg.img_size,\n",
    "                patch_size=cfg.patch_size,\n",
    "                in_chans=cfg.in_chans,\n",
    "                mask_ratio=cfg.mask_ratio,\n",
    "                norm_pix_loss=cfg.norm_pix_loss,\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(\"backbone must be vit_base or vit_large\")\n",
    "\n",
    "    def training_step(self, batch: Any, batch_idx: int) -> torch.Tensor:\n",
    "        imgs = batch[0] if isinstance(batch, (tuple, list)) else batch\n",
    "        loss, _, _ = self.model(imgs)\n",
    "        self.log(\"train/loss\", loss, prog_bar=True, on_step=True, on_epoch=True)\n",
    "        self.log(\"train/loss_epoch\", loss, prog_bar=False, on_step=False, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch: Any, batch_idx: int) -> Dict[str, torch.Tensor]:\n",
    "        imgs = batch[0] if isinstance(batch, (tuple, list)) else batch\n",
    "        loss, _, _ = self.model(imgs)\n",
    "        self.log(\"val/loss\", loss, prog_bar=True, on_step=False, on_epoch=True)\n",
    "        return {\"val_loss\": loss}\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        opt = torch.optim.AdamW(self.parameters(), lr=self.cfg.lr, weight_decay=self.cfg.weight_decay)\n",
    "        return opt\n",
    "\n",
    "    def get_encoder_state_dict(self) -> Dict[str, torch.Tensor]:\n",
    "        sd = self.model.state_dict()\n",
    "        keys_to_drop = [k for k in sd.keys() if k.startswith(\"decoder_\") or k in {\"mask_token\"}]\n",
    "        for k in keys_to_drop:\n",
    "            sd.pop(k, None)\n",
    "        return sd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b604b7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-30T14:34:35.614313Z",
     "iopub.status.busy": "2025-12-30T14:34:35.614186Z",
     "iopub.status.idle": "2025-12-30T14:34:35.617730Z",
     "shell.execute_reply": "2025-12-30T14:34:35.617365Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.006979,
     "end_time": "2025-12-30T14:34:35.618529",
     "exception": false,
     "start_time": "2025-12-30T14:34:35.611550",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing visualizations.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile visualizations.py\n",
    "from __future__ import annotations\n",
    "\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def mae_reconstruction_triplet(\n",
    "    *,\n",
    "    mae_model,\n",
    "    imgs: torch.Tensor,\n",
    "    device: Optional[torch.device] = None,\n",
    ") -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "    device = device or imgs.device\n",
    "    mae_model.eval()\n",
    "\n",
    "    imgs = imgs.to(device)\n",
    "    _, pred, mask = mae_model(imgs)\n",
    "\n",
    "    patches = mae_model.patchify(imgs)\n",
    "    mask_ = mask.unsqueeze(-1)\n",
    "    visible_patches = patches * (1.0 - mask_)\n",
    "    masked_vis = mae_model.unpatchify(visible_patches)\n",
    "\n",
    "    recon_patches = patches * (1.0 - mask_) + pred * mask_\n",
    "    recon_img = mae_model.unpatchify(recon_patches)\n",
    "\n",
    "    return imgs, masked_vis, recon_img\n",
    "\n",
    "\n",
    "def simple_input_gradient_saliency(\n",
    "    *,\n",
    "    model,\n",
    "    imgs: torch.Tensor,\n",
    "    targets: Optional[torch.Tensor] = None,\n",
    ") -> torch.Tensor:\n",
    "    model.eval()\n",
    "    imgs = imgs.requires_grad_(True)\n",
    "\n",
    "    logits = model(imgs)\n",
    "    if targets is None:\n",
    "        targets = logits.argmax(dim=1)\n",
    "    sel = logits.gather(1, targets.view(-1, 1)).sum()\n",
    "\n",
    "    sel.backward()\n",
    "    grads = imgs.grad.detach().abs().mean(dim=1, keepdim=True)\n",
    "\n",
    "    n = grads.shape[0]\n",
    "    grads_flat = grads.view(n, -1)\n",
    "    mins = grads_flat.min(dim=1).values.view(n, 1, 1, 1)\n",
    "    maxs = grads_flat.max(dim=1).values.view(n, 1, 1, 1)\n",
    "    sal = (grads - mins) / (maxs - mins + 1e-8)\n",
    "    return sal\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def plot_mae_single_image_reconstruction(\n",
    "    *,\n",
    "    mae_model,\n",
    "    image: \"torch.Tensor | Image.Image\",\n",
    "    device: Optional[torch.device] = None,\n",
    "    save_path: Optional[str] = None,\n",
    "    title: Optional[str] = None,\n",
    "):\n",
    "    import matplotlib.pyplot as plt\n",
    "    import torch.nn.functional as F\n",
    "\n",
    "    mae_model.eval()\n",
    "    if device is None:\n",
    "        device = next(mae_model.parameters()).device\n",
    "\n",
    "    if isinstance(image, Image.Image):\n",
    "        import torchvision.transforms.functional as TF\n",
    "\n",
    "        img_t = TF.to_tensor(image.convert(\"RGB\"))\n",
    "    else:\n",
    "        img_t = image\n",
    "\n",
    "    if img_t.dim() != 3:\n",
    "        raise ValueError(\"Expected a single image tensor of shape (C,H,W)\")\n",
    "\n",
    "    imgs = img_t.unsqueeze(0).to(device)\n",
    "\n",
    "    expected = None\n",
    "    if hasattr(mae_model, \"cfg\") and hasattr(mae_model.cfg, \"img_size\"):\n",
    "        expected = int(mae_model.cfg.img_size)\n",
    "    elif hasattr(mae_model, \"patch_embed\") and hasattr(mae_model.patch_embed, \"img_size\"):\n",
    "        pe_size = mae_model.patch_embed.img_size\n",
    "        expected = int(pe_size[0] if isinstance(pe_size, (tuple, list)) else pe_size)\n",
    "\n",
    "    if expected is not None:\n",
    "        _, _, h, w = imgs.shape\n",
    "        if h != expected or w != expected:\n",
    "            imgs = F.interpolate(imgs, size=(expected, expected), mode=\"bilinear\", align_corners=False)\n",
    "\n",
    "    orig, masked, recon = mae_reconstruction_triplet(mae_model=mae_model, imgs=imgs, device=device)\n",
    "\n",
    "    orig0 = orig[0].detach().cpu().clamp(0, 1)\n",
    "    masked0 = masked[0].detach().cpu().clamp(0, 1)\n",
    "    recon0 = recon[0].detach().cpu().clamp(0, 1)\n",
    "\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(10, 3.5))\n",
    "    for ax, im, t in zip(\n",
    "        axs,\n",
    "        [orig0, masked0, recon0],\n",
    "        [\"original\", \"corrupted (masked)\", \"reconstruction\"],\n",
    "    ):\n",
    "        ax.imshow(im.permute(1, 2, 0))\n",
    "        ax.set_title(t)\n",
    "        ax.axis(\"off\")\n",
    "\n",
    "    if title:\n",
    "        fig.suptitle(title)\n",
    "\n",
    "    fig.tight_layout()\n",
    "    if save_path:\n",
    "        fig.savefig(save_path, dpi=150)\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f8a64ee",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-30T14:34:35.623878Z",
     "iopub.status.busy": "2025-12-30T14:34:35.623718Z",
     "iopub.status.idle": "2025-12-30T14:34:35.633157Z",
     "shell.execute_reply": "2025-12-30T14:34:35.632789Z"
    },
    "papermill": {
     "duration": 0.013285,
     "end_time": "2025-12-30T14:34:35.633901",
     "exception": false,
     "start_time": "2025-12-30T14:34:35.620616",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing main.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile main.py\n",
    "from __future__ import annotations\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint\n",
    "\n",
    "from pretrain_module import MAEPretrainModule, PretrainConfig\n",
    "from eval_module import ViTClassifier, EvalConfig\n",
    "from vision_transformer import build_vit_encoder, make_vit_config\n",
    "from visualizations import mae_reconstruction_triplet\n",
    "from visualizations import plot_mae_single_image_reconstruction\n",
    "\n",
    "\n",
    "IMG_EXTS = {\".jpg\", \".jpeg\", \".png\", \".bmp\", \".tif\", \".tiff\"}\n",
    "\n",
    "\n",
    "def _now() -> str:\n",
    "    return datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "\n",
    "def _print_kv(title: str, kv: Dict[str, object]) -> None:\n",
    "    print(f\"\\n[{_now()}] {title}\")\n",
    "    for k, v in kv.items():\n",
    "        print(f\"  - {k}: {v}\")\n",
    "\n",
    "\n",
    "def _env_summary() -> Dict[str, object]:\n",
    "    return {\n",
    "        \"python\": os.sys.version.split()[0],\n",
    "        \"torch\": torch.__version__,\n",
    "        \"cuda_available\": torch.cuda.is_available(),\n",
    "        \"cuda_device_count\": torch.cuda.device_count() if torch.cuda.is_available() else 0,\n",
    "        \"device_name\": torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"cpu\",\n",
    "    }\n",
    "\n",
    "\n",
    "def _dataloader_common_kwargs(num_workers: int) -> Dict[str, object]:\n",
    "    return {\n",
    "        \"num_workers\": num_workers,\n",
    "        \"pin_memory\": True,\n",
    "        \"persistent_workers\": bool(num_workers and num_workers > 0),\n",
    "    }\n",
    "\n",
    "\n",
    "def _build_trainer(*, out_dir: str, epochs: int, precision: str, has_val: bool) -> pl.Trainer:\n",
    "    ckpt_dir = os.path.join(out_dir, \"checkpoints\")\n",
    "    os.makedirs(ckpt_dir, exist_ok=True)\n",
    "\n",
    "    monitor_key = \"val/loss\" if has_val else \"train/loss_epoch\"\n",
    "    filename_metric = monitor_key.replace(\"/\", \"_\")\n",
    "    callbacks = [\n",
    "        LearningRateMonitor(logging_interval=\"step\"),\n",
    "        ModelCheckpoint(\n",
    "            dirpath=ckpt_dir,\n",
    "            filename=\"{epoch:03d}-{step:06d}\" + f\"-{{{filename_metric}:.4f}}\",\n",
    "            monitor=monitor_key,\n",
    "            mode=\"min\",\n",
    "            save_last=True,\n",
    "            save_top_k=1,\n",
    "            auto_insert_metric_name=False,\n",
    "        ),\n",
    "        _EpochMetricsAndPlotsCallback(out_dir=out_dir, max_epochs=epochs, has_val=has_val),\n",
    "    ]\n",
    "\n",
    "    return pl.Trainer(\n",
    "        max_epochs=epochs,\n",
    "        accelerator=\"auto\",\n",
    "        devices=\"auto\",\n",
    "        precision=precision,\n",
    "        default_root_dir=out_dir,\n",
    "        log_every_n_steps=10,\n",
    "        enable_progress_bar=False,\n",
    "        enable_checkpointing=True,\n",
    "        callbacks=callbacks,\n",
    "    )\n",
    "\n",
    "\n",
    "def _to_float(x) -> Optional[float]:\n",
    "    if x is None:\n",
    "        return None\n",
    "    try:\n",
    "        if hasattr(x, \"detach\"):\n",
    "            return float(x.detach().cpu().item())\n",
    "        return float(x)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "\n",
    "def _pick_metric(metrics: Dict[str, object], names: List[str]) -> Optional[float]:\n",
    "    for n in names:\n",
    "        if n in metrics:\n",
    "            v = _to_float(metrics[n])\n",
    "            if v is not None:\n",
    "                return v\n",
    "    return None\n",
    "\n",
    "\n",
    "class _EpochMetricsAndPlotsCallback(pl.Callback):\n",
    "    def __init__(self, *, out_dir: str, max_epochs: int, has_val: bool):\n",
    "        super().__init__()\n",
    "        self.out_dir = out_dir\n",
    "        self.max_epochs = max_epochs\n",
    "        self.has_val = has_val\n",
    "\n",
    "        self.train_loss: List[float] = []\n",
    "        self.val_loss: List[float] = []\n",
    "        self.train_acc: List[float] = []\n",
    "        self.val_acc: List[float] = []\n",
    "\n",
    "        self._last_train_loss: Optional[float] = None\n",
    "        self._last_train_acc: Optional[float] = None\n",
    "\n",
    "    def on_train_epoch_start(self, trainer: pl.Trainer, pl_module: pl.LightningModule) -> None:\n",
    "        if trainer.sanity_checking:\n",
    "            return\n",
    "        e = int(trainer.current_epoch)\n",
    "        print(f\"\\n[{_now()}] Epoch {e}/{self.max_epochs - 1}\")\n",
    "\n",
    "    def on_train_epoch_end(self, trainer: pl.Trainer, pl_module: pl.LightningModule) -> None:\n",
    "        if trainer.sanity_checking:\n",
    "            return\n",
    "\n",
    "        metrics = trainer.callback_metrics\n",
    "        t_loss = _pick_metric(metrics, [\"train/loss_epoch\", \"train/loss\"])  \n",
    "        t_acc = _pick_metric(metrics, [\"train/acc_epoch\", \"train/acc\"])\n",
    "\n",
    "        self._last_train_loss = t_loss\n",
    "        self._last_train_acc = t_acc\n",
    "\n",
    "        if t_loss is not None:\n",
    "            self.train_loss.append(t_loss)\n",
    "        if t_acc is not None:\n",
    "            self.train_acc.append(t_acc)\n",
    "\n",
    "        if not self.has_val:\n",
    "            parts = []\n",
    "            if t_loss is not None:\n",
    "                parts.append(f\"train_loss={t_loss:.6f}\")\n",
    "            if t_acc is not None:\n",
    "                parts.append(f\"train_acc={t_acc:.4f}\")\n",
    "            if parts:\n",
    "                print(f\"[{_now()}] \" + \"  \".join(parts))\n",
    "\n",
    "    def on_validation_epoch_end(self, trainer: pl.Trainer, pl_module: pl.LightningModule) -> None:\n",
    "        if trainer.sanity_checking:\n",
    "            return\n",
    "\n",
    "        metrics = trainer.callback_metrics\n",
    "        v_loss = _pick_metric(metrics, [\"val/loss\", \"val/loss_epoch\"])\n",
    "        v_acc = _pick_metric(metrics, [\"val/acc\", \"val/acc_epoch\"])\n",
    "\n",
    "        if v_loss is not None:\n",
    "            self.val_loss.append(v_loss)\n",
    "        if v_acc is not None:\n",
    "            self.val_acc.append(v_acc)\n",
    "\n",
    "        parts = []\n",
    "        if self._last_train_loss is not None:\n",
    "            parts.append(f\"train_loss={self._last_train_loss:.6f}\")\n",
    "        if self._last_train_acc is not None:\n",
    "            parts.append(f\"train_acc={self._last_train_acc:.4f}\")\n",
    "        if v_loss is not None:\n",
    "            parts.append(f\"val_loss={v_loss:.6f}\")\n",
    "        if v_acc is not None:\n",
    "            parts.append(f\"val_acc={v_acc:.4f}\")\n",
    "        if parts:\n",
    "            print(f\"[{_now()}] \" + \"  \".join(parts))\n",
    "\n",
    "    def on_fit_end(self, trainer: pl.Trainer, pl_module: pl.LightningModule) -> None:\n",
    "        import os\n",
    "        import csv\n",
    "\n",
    "        os.makedirs(self.out_dir, exist_ok=True)\n",
    "\n",
    "        try:\n",
    "            rows = max(len(self.train_loss), len(self.val_loss), len(self.train_acc), len(self.val_acc))\n",
    "            csv_path = os.path.join(self.out_dir, \"metrics.csv\")\n",
    "            with open(csv_path, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "                writer = csv.writer(f)\n",
    "                writer.writerow([\"epoch\", \"train_loss\", \"val_loss\", \"train_acc\", \"val_acc\"])\n",
    "                for e in range(rows):\n",
    "                    writer.writerow(\n",
    "                        [\n",
    "                            e,\n",
    "                            self.train_loss[e] if e < len(self.train_loss) else \"\",\n",
    "                            self.val_loss[e] if e < len(self.val_loss) else \"\",\n",
    "                            self.train_acc[e] if e < len(self.train_acc) else \"\",\n",
    "                            self.val_acc[e] if e < len(self.val_acc) else \"\",\n",
    "                        ]\n",
    "                    )\n",
    "            print(f\"[{_now()}] Saved metrics CSV: {csv_path}\")\n",
    "        except Exception as exc:\n",
    "            print(f\"[{_now()}] Could not write metrics.csv: {exc}\")\n",
    "\n",
    "        if self.train_loss or self.val_loss:\n",
    "            try:\n",
    "                import matplotlib.pyplot as plt\n",
    "\n",
    "                fig = plt.figure(figsize=(7, 4))\n",
    "                ax = fig.add_subplot(1, 1, 1)\n",
    "                if self.train_loss:\n",
    "                    ax.plot(range(len(self.train_loss)), self.train_loss, label=\"train_loss\")\n",
    "                if self.val_loss:\n",
    "                    ax.plot(range(len(self.val_loss)), self.val_loss, label=\"val_loss\")\n",
    "                ax.set_title(\"Loss vs Epoch\")\n",
    "                ax.set_xlabel(\"epoch\")\n",
    "                ax.set_ylabel(\"loss\")\n",
    "                ax.grid(True, alpha=0.3)\n",
    "                ax.legend()\n",
    "                out_path = os.path.join(self.out_dir, \"loss_curve.png\")\n",
    "                fig.tight_layout()\n",
    "                fig.savefig(out_path, dpi=150)\n",
    "                plt.close(fig)\n",
    "                print(f\"[{_now()}] Saved loss plot: {out_path}\")\n",
    "            except Exception as exc:\n",
    "                print(f\"[{_now()}] Could not write loss plot: {exc}\")\n",
    "\n",
    "        if self.train_acc or self.val_acc:\n",
    "            try:\n",
    "                import matplotlib.pyplot as plt\n",
    "\n",
    "                fig = plt.figure(figsize=(7, 4))\n",
    "                ax = fig.add_subplot(1, 1, 1)\n",
    "                if self.train_acc:\n",
    "                    ax.plot(range(len(self.train_acc)), self.train_acc, label=\"train_acc\")\n",
    "                if self.val_acc:\n",
    "                    ax.plot(range(len(self.val_acc)), self.val_acc, label=\"val_acc\")\n",
    "                ax.set_title(\"Accuracy vs Epoch\")\n",
    "                ax.set_xlabel(\"epoch\")\n",
    "                ax.set_ylabel(\"accuracy\")\n",
    "                ax.grid(True, alpha=0.3)\n",
    "                ax.legend()\n",
    "                out_path = os.path.join(self.out_dir, \"acc_curve.png\")\n",
    "                fig.tight_layout()\n",
    "                fig.savefig(out_path, dpi=150)\n",
    "                plt.close(fig)\n",
    "                print(f\"[{_now()}] Saved accuracy plot: {out_path}\")\n",
    "            except Exception as exc:\n",
    "                print(f\"[{_now()}] Could not write accuracy plot: {exc}\")\n",
    "\n",
    "\n",
    "def _summarize_labeled_dataset(ds: \"LabeledImageFolder\") -> Dict[str, object]:\n",
    "    idx_to_class = {v: k for k, v in ds.class_to_idx.items()}\n",
    "    sample_preview = []\n",
    "    for i in range(min(3, len(ds.samples))):\n",
    "        p, y = ds.samples[i]\n",
    "        sample_preview.append({\"path\": p, \"label\": int(y), \"class\": idx_to_class.get(int(y), \"?\")})\n",
    "    return {\n",
    "        \"root\": ds.root,\n",
    "        \"num_samples\": len(ds),\n",
    "        \"num_classes\": len(ds.class_to_idx),\n",
    "        \"class_to_idx\": ds.class_to_idx,\n",
    "        \"sample_preview\": sample_preview,\n",
    "    }\n",
    "\n",
    "\n",
    "def _summarize_unlabeled_dataset(ds: \"UnlabeledImageFolder\") -> Dict[str, object]:\n",
    "    sample_preview = ds.paths[:3]\n",
    "    return {\n",
    "        \"root\": ds.root,\n",
    "        \"num_samples\": len(ds),\n",
    "        \"sample_preview\": sample_preview,\n",
    "    }\n",
    "\n",
    "\n",
    "class UnlabeledImageFolder(Dataset):\n",
    "    \"\"\"Recursively loads images from a folder (no labels required).\"\"\"\n",
    "\n",
    "    def __init__(self, root: str, transform=None):\n",
    "        self.root = root\n",
    "        self.transform = transform\n",
    "        self.paths: List[str] = []\n",
    "        for dirpath, _, filenames in os.walk(root):\n",
    "            for fn in filenames:\n",
    "                ext = os.path.splitext(fn)[1].lower()\n",
    "                if ext in IMG_EXTS:\n",
    "                    self.paths.append(os.path.join(dirpath, fn))\n",
    "        if len(self.paths) == 0:\n",
    "            raise ValueError(f\"No images found under: {root}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        path = self.paths[idx]\n",
    "        img = Image.open(path).convert(\"RGB\")\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        return img\n",
    "\n",
    "\n",
    "class LabeledImageFolder(Dataset):\n",
    "    \"\"\"Labeled folder format:\n",
    "\n",
    "    root/\n",
    "      class_a/xxx.png\n",
    "      class_b/yyy.png\n",
    "\n",
    "    Returns (image_tensor, class_index)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, root: str, transform=None):\n",
    "        self.root = root\n",
    "        self.transform = transform\n",
    "\n",
    "        class_names = [d for d in os.listdir(root) if os.path.isdir(os.path.join(root, d))]\n",
    "        class_names.sort()\n",
    "        if not class_names:\n",
    "            raise ValueError(f\"No class subfolders found under: {root}\")\n",
    "        self.class_to_idx = {name: i for i, name in enumerate(class_names)}\n",
    "\n",
    "        self.samples: List[Tuple[str, int]] = []\n",
    "        for cls in class_names:\n",
    "            cls_dir = os.path.join(root, cls)\n",
    "            for dirpath, _, filenames in os.walk(cls_dir):\n",
    "                for fn in filenames:\n",
    "                    ext = os.path.splitext(fn)[1].lower()\n",
    "                    if ext in IMG_EXTS:\n",
    "                        self.samples.append((os.path.join(dirpath, fn), self.class_to_idx[cls]))\n",
    "        if len(self.samples) == 0:\n",
    "            raise ValueError(f\"No images found under: {root}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        path, y = self.samples[idx]\n",
    "        img = Image.open(path).convert(\"RGB\")\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        return img, torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "\n",
    "def make_transforms(img_size: int, *, mode: str):\n",
    "    if mode == \"pretrain\":\n",
    "        return transforms.Compose(\n",
    "            [\n",
    "                transforms.RandomResizedCrop(img_size, scale=(0.8, 1.0)),\n",
    "                transforms.ToTensor(),\n",
    "            ]\n",
    "        )\n",
    "    if mode == \"finetune\":\n",
    "        return transforms.Compose(\n",
    "            [\n",
    "                transforms.RandomResizedCrop(img_size, scale=(0.6, 1.0)),\n",
    "                transforms.RandomHorizontalFlip(p=0.5),\n",
    "                transforms.ColorJitter(brightness=0.1, contrast=0.1),\n",
    "                transforms.ToTensor(),\n",
    "            ]\n",
    "        )\n",
    "    if mode == \"eval\":\n",
    "        return transforms.Compose(\n",
    "            [\n",
    "                transforms.Resize(img_size),\n",
    "                transforms.CenterCrop(img_size),\n",
    "                transforms.ToTensor(),\n",
    "            ]\n",
    "        )\n",
    "    raise ValueError(\"Unknown mode\")\n",
    "\n",
    "\n",
    "def cmd_pretrain(args) -> None:\n",
    "    _print_kv(\"Environment\", _env_summary())\n",
    "    cfg = PretrainConfig(\n",
    "        backbone=args.backbone,\n",
    "        img_size=args.img_size,\n",
    "        patch_size=args.patch_size,\n",
    "        in_chans=3,\n",
    "        mask_ratio=args.mask_ratio,\n",
    "        norm_pix_loss=args.norm_pix_loss,\n",
    "        lr=args.lr,\n",
    "        weight_decay=args.weight_decay,\n",
    "    )\n",
    "    _print_kv(\n",
    "        \"Pretrain config\",\n",
    "        {\n",
    "            \"backbone\": cfg.backbone,\n",
    "            \"img_size\": cfg.img_size,\n",
    "            \"patch_size\": cfg.patch_size,\n",
    "            \"mask_ratio\": cfg.mask_ratio,\n",
    "            \"norm_pix_loss\": cfg.norm_pix_loss,\n",
    "            \"batch_size\": args.batch_size,\n",
    "            \"epochs\": args.epochs,\n",
    "            \"lr\": cfg.lr,\n",
    "            \"weight_decay\": cfg.weight_decay,\n",
    "            \"precision\": args.precision,\n",
    "            \"num_workers\": args.num_workers,\n",
    "            \"out_dir\": args.out_dir,\n",
    "        },\n",
    "    )\n",
    "\n",
    "    dm_train = UnlabeledImageFolder(args.train_dir, transform=make_transforms(args.img_size, mode=\"pretrain\"))\n",
    "    _print_kv(\"Pretrain dataset (train)\", _summarize_unlabeled_dataset(dm_train))\n",
    "    train_loader = DataLoader(\n",
    "        dm_train,\n",
    "        batch_size=args.batch_size,\n",
    "        shuffle=True,\n",
    "        **_dataloader_common_kwargs(args.num_workers),\n",
    "    )\n",
    "\n",
    "    val_loader = None\n",
    "    if args.val_dir:\n",
    "        dm_val = UnlabeledImageFolder(args.val_dir, transform=make_transforms(args.img_size, mode=\"eval\"))\n",
    "        _print_kv(\"Pretrain dataset (val)\", _summarize_unlabeled_dataset(dm_val))\n",
    "        val_loader = DataLoader(\n",
    "            dm_val,\n",
    "            batch_size=args.batch_size,\n",
    "            shuffle=False,\n",
    "            **_dataloader_common_kwargs(args.num_workers),\n",
    "        )\n",
    "\n",
    "    module = MAEPretrainModule(cfg)\n",
    "    trainer = _build_trainer(out_dir=args.out_dir, epochs=args.epochs, precision=args.precision, has_val=val_loader is not None)\n",
    "\n",
    "    print(f\"\\n[{_now()}] Starting MAE pretraining...\")\n",
    "    trainer.fit(module, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
    "\n",
    "    os.makedirs(args.out_dir, exist_ok=True)\n",
    "\n",
    "    ckpt_cb = next((c for c in trainer.callbacks if isinstance(c, ModelCheckpoint)), None)\n",
    "    best_path = ckpt_cb.best_model_path if ckpt_cb is not None else \"\"\n",
    "    if best_path and os.path.exists(best_path):\n",
    "        ckpt_path = os.path.join(args.out_dir, \"mae_pretrained.ckpt\")\n",
    "        shutil.copy2(best_path, ckpt_path)\n",
    "        print(f\"[{_now()}] Selected best checkpoint: {best_path}\")\n",
    "        print(f\"[{_now()}] Saved best-as-default: {ckpt_path}\")\n",
    "    else:\n",
    "        ckpt_path = os.path.join(args.out_dir, \"mae_pretrained.ckpt\")\n",
    "        trainer.save_checkpoint(ckpt_path)\n",
    "        print(f\"[{_now()}] Saved checkpoint: {ckpt_path}\")\n",
    "    print(f\"[{_now()}] Lightning checkpoints: {os.path.join(args.out_dir, 'checkpoints')}\")\n",
    "\n",
    "\n",
    "def _load_encoder_from_mae_ckpt(ckpt_path: str, *, backbone: str, img_size: int, patch_size: int) -> torch.nn.Module:\n",
    "    if backbone == \"vit_base\":\n",
    "        vit_cfg = make_vit_config(\"base\", img_size=img_size, patch_size=patch_size, in_chans=3)\n",
    "    elif backbone == \"vit_large\":\n",
    "        vit_cfg = make_vit_config(\"large\", img_size=img_size, patch_size=patch_size, in_chans=3)\n",
    "    else:\n",
    "        raise ValueError(\"backbone must be vit_base or vit_large\")\n",
    "\n",
    "    encoder = build_vit_encoder(vit_cfg)\n",
    "\n",
    "    ckpt = torch.load(ckpt_path, map_location=\"cpu\")\n",
    "    state = ckpt.get(\"state_dict\", ckpt)\n",
    "    \n",
    "    enc_state = {}\n",
    "    for k, v in state.items():\n",
    "        if k.startswith(\"model.\"):\n",
    "            kk = k[len(\"model.\") :]\n",
    "        else:\n",
    "            kk = k\n",
    "\n",
    "        if kk.startswith(\"patch_embed\") or kk.startswith(\"blocks\") or kk.startswith(\"norm\") or kk.startswith(\"pos_embed\") or kk.startswith(\"cls_token\"):\n",
    "            enc_state[kk] = v\n",
    "\n",
    "    missing, unexpected = encoder.load_state_dict(enc_state, strict=False)\n",
    "    if unexpected:\n",
    "        print(\"Unexpected keys:\", unexpected)\n",
    "    if missing:\n",
    "        print(\"Missing keys (often OK):\", missing)\n",
    "\n",
    "    return encoder\n",
    "\n",
    "\n",
    "def cmd_finetune(args) -> None:\n",
    "    _print_kv(\"Environment\", _env_summary())\n",
    "    train_ds = LabeledImageFolder(args.train_dir, transform=make_transforms(args.img_size, mode=\"finetune\"))\n",
    "    val_ds = LabeledImageFolder(args.val_dir, transform=make_transforms(args.img_size, mode=\"eval\"))\n",
    "\n",
    "    _print_kv(\"Finetune dataset (train)\", _summarize_labeled_dataset(train_ds))\n",
    "    _print_kv(\"Finetune dataset (val)\", _summarize_labeled_dataset(val_ds))\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_ds,\n",
    "        batch_size=args.batch_size,\n",
    "        shuffle=True,\n",
    "        **_dataloader_common_kwargs(args.num_workers),\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_ds,\n",
    "        batch_size=args.batch_size,\n",
    "        shuffle=False,\n",
    "        **_dataloader_common_kwargs(args.num_workers),\n",
    "    )\n",
    "\n",
    "    num_classes = len(train_ds.class_to_idx)\n",
    "\n",
    "    _print_kv(\n",
    "        \"Finetune config\",\n",
    "        {\n",
    "            \"init\": args.init,\n",
    "            \"mae_ckpt\": args.mae_ckpt if args.init == 'mae' else None,\n",
    "            \"backbone\": args.backbone,\n",
    "            \"img_size\": args.img_size,\n",
    "            \"patch_size\": args.patch_size,\n",
    "            \"num_classes\": num_classes,\n",
    "            \"freeze_encoder\": bool(args.freeze_encoder),\n",
    "            \"encoder_lr\": args.encoder_lr,\n",
    "            \"batch_size\": args.batch_size,\n",
    "            \"epochs\": args.epochs,\n",
    "            \"lr\": args.lr,\n",
    "            \"weight_decay\": args.weight_decay,\n",
    "            \"precision\": args.precision,\n",
    "            \"num_workers\": args.num_workers,\n",
    "            \"out_dir\": args.out_dir,\n",
    "        },\n",
    "    )\n",
    "\n",
    "    if args.init == \"mae\":\n",
    "        if not args.mae_ckpt:\n",
    "            raise ValueError(\"--mae-ckpt is required when --init mae\")\n",
    "        print(f\"[{_now()}] Loading encoder weights from MAE checkpoint: {args.mae_ckpt}\")\n",
    "        encoder = _load_encoder_from_mae_ckpt(args.mae_ckpt, backbone=args.backbone, img_size=args.img_size, patch_size=args.patch_size)\n",
    "    elif args.init == \"imagenet\":\n",
    "        import timm\n",
    "\n",
    "        model_name = \"vit_base_patch16_224\" if args.backbone == \"vit_base\" else \"vit_large_patch16_224\"\n",
    "        print(f\"[{_now()}] Loading ImageNet pretrained encoder via timm: {model_name}\")\n",
    "        encoder = timm.create_model(model_name, pretrained=True, num_classes=0)\n",
    "    else:\n",
    "        raise ValueError(\"--init must be mae or imagenet\")\n",
    "\n",
    "    module = ViTClassifier(\n",
    "        encoder=encoder,\n",
    "        cfg=EvalConfig(\n",
    "            num_classes=num_classes,\n",
    "            lr=args.lr,\n",
    "            encoder_lr=args.encoder_lr,\n",
    "            weight_decay=args.weight_decay,\n",
    "            freeze_encoder=bool(args.freeze_encoder),\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    trainer = _build_trainer(out_dir=args.out_dir, epochs=args.epochs, precision=args.precision, has_val=True)\n",
    "\n",
    "    print(f\"\\n[{_now()}] Starting fine-tuning...\")\n",
    "    trainer.fit(module, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
    "\n",
    "    os.makedirs(args.out_dir, exist_ok=True)\n",
    "    ckpt_path = os.path.join(args.out_dir, \"finetuned.ckpt\")\n",
    "    trainer.save_checkpoint(ckpt_path)\n",
    "    print(f\"[{_now()}] Saved checkpoint: {ckpt_path}\")\n",
    "    print(f\"[{_now()}] Lightning checkpoints: {os.path.join(args.out_dir, 'checkpoints')}\")\n",
    "\n",
    "\n",
    "def cmd_finetune_resnet(args) -> None:\n",
    "    _print_kv(\"Environment\", _env_summary())\n",
    "    train_ds = LabeledImageFolder(args.train_dir, transform=make_transforms(args.img_size, mode=\"finetune\"))\n",
    "    val_ds = LabeledImageFolder(args.val_dir, transform=make_transforms(args.img_size, mode=\"eval\"))\n",
    "\n",
    "    _print_kv(\"Finetune (ResNet) dataset (train)\", _summarize_labeled_dataset(train_ds))\n",
    "    _print_kv(\"Finetune (ResNet) dataset (val)\", _summarize_labeled_dataset(val_ds))\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_ds,\n",
    "        batch_size=args.batch_size,\n",
    "        shuffle=True,\n",
    "        **_dataloader_common_kwargs(args.num_workers),\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_ds,\n",
    "        batch_size=args.batch_size,\n",
    "        shuffle=False,\n",
    "        **_dataloader_common_kwargs(args.num_workers),\n",
    "    )\n",
    "\n",
    "    num_classes = len(train_ds.class_to_idx)\n",
    "    _print_kv(\n",
    "        \"Finetune (ResNet) config\",\n",
    "        {\n",
    "            \"arch\": args.arch,\n",
    "            \"pretrained\": bool(args.pretrained),\n",
    "            \"freeze_encoder\": bool(args.freeze_encoder),\n",
    "            \"encoder_lr\": args.encoder_lr,\n",
    "            \"img_size\": args.img_size,\n",
    "            \"num_classes\": num_classes,\n",
    "            \"batch_size\": args.batch_size,\n",
    "            \"epochs\": args.epochs,\n",
    "            \"lr\": args.lr,\n",
    "            \"weight_decay\": args.weight_decay,\n",
    "            \"precision\": args.precision,\n",
    "            \"num_workers\": args.num_workers,\n",
    "            \"out_dir\": args.out_dir,\n",
    "        },\n",
    "    )\n",
    "\n",
    "    from resnet_eval_module import ResNetClassifier, ResNetConfig\n",
    "\n",
    "    module = ResNetClassifier(\n",
    "        cfg=ResNetConfig(\n",
    "            arch=args.arch,\n",
    "            pretrained=bool(args.pretrained),\n",
    "            num_classes=num_classes,\n",
    "            lr=args.lr,\n",
    "            encoder_lr=args.encoder_lr,\n",
    "            weight_decay=args.weight_decay,\n",
    "            freeze_encoder=bool(args.freeze_encoder),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    trainer = _build_trainer(out_dir=args.out_dir, epochs=args.epochs, precision=args.precision, has_val=True)\n",
    "    print(f\"\\n[{_now()}] Starting ResNet linear-probe...\")\n",
    "    trainer.fit(module, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
    "\n",
    "    os.makedirs(args.out_dir, exist_ok=True)\n",
    "    ckpt_path = os.path.join(args.out_dir, \"finetuned.ckpt\")\n",
    "    trainer.save_checkpoint(ckpt_path)\n",
    "    print(f\"[{_now()}] Saved checkpoint: {ckpt_path}\")\n",
    "    print(f\"[{_now()}] Lightning checkpoints: {os.path.join(args.out_dir, 'checkpoints')}\")\n",
    "\n",
    "\n",
    "def cmd_visualize(args) -> None:\n",
    "    from pretrain_module import MAEPretrainModule, PretrainConfig\n",
    "\n",
    "    cfg = PretrainConfig(\n",
    "        backbone=args.backbone,\n",
    "        img_size=args.img_size,\n",
    "        patch_size=args.patch_size,\n",
    "        mask_ratio=args.mask_ratio,\n",
    "    )\n",
    "    module = MAEPretrainModule(cfg)\n",
    "    ckpt = torch.load(args.ckpt, map_location=\"cpu\")\n",
    "    module.load_state_dict(ckpt[\"state_dict\"], strict=True)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    module.model.to(device)\n",
    "\n",
    "    img = Image.open(args.image_path).convert(\"RGB\")\n",
    "\n",
    "    fig = plot_mae_single_image_reconstruction(\n",
    "        mae_model=module.model,\n",
    "        image=img,\n",
    "        device=device,\n",
    "        save_path=args.out_path,\n",
    "        title=args.title,\n",
    "    )\n",
    "\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def cmd_reconstruct(args) -> None:\n",
    "    from pretrain_module import MAEPretrainModule, PretrainConfig\n",
    "\n",
    "    cfg = PretrainConfig(backbone=args.backbone, img_size=args.img_size, patch_size=args.patch_size, mask_ratio=args.mask_ratio)\n",
    "    module = MAEPretrainModule(cfg)\n",
    "\n",
    "    ckpt = torch.load(args.ckpt, map_location=\"cpu\")\n",
    "    module.load_state_dict(ckpt[\"state_dict\"], strict=True)\n",
    "\n",
    "    ds = UnlabeledImageFolder(args.images_dir, transform=make_transforms(args.img_size, mode=\"eval\"))\n",
    "    loader = DataLoader(ds, batch_size=args.batch_size, shuffle=False, num_workers=args.num_workers)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    module.model.to(device)\n",
    "\n",
    "    _print_kv(\n",
    "        \"Reconstruction run\",\n",
    "        {\n",
    "            \"images_dir\": args.images_dir,\n",
    "            \"num_images\": len(ds),\n",
    "            \"ckpt\": args.ckpt,\n",
    "            \"device\": str(device),\n",
    "            \"mask_ratio\": args.mask_ratio,\n",
    "            \"out_dir\": args.out_dir,\n",
    "        },\n",
    "    )\n",
    "\n",
    "    os.makedirs(args.out_dir, exist_ok=True)\n",
    "\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    for i, imgs in enumerate(loader):\n",
    "        if i >= args.max_batches:\n",
    "            break\n",
    "        orig, masked, recon = mae_reconstruction_triplet(mae_model=module.model, imgs=imgs, device=device)\n",
    "        for b in range(min(orig.shape[0], args.max_items)):\n",
    "            fig, axs = plt.subplots(1, 3, figsize=(9, 3))\n",
    "            for ax, im, title in zip(\n",
    "                axs,\n",
    "                [orig[b], masked[b], recon[b]],\n",
    "                [\"original\", \"masked\", \"recon\"],\n",
    "            ):\n",
    "                ax.imshow(im.permute(1, 2, 0).detach().cpu().clamp(0, 1))\n",
    "                ax.set_title(title)\n",
    "                ax.axis(\"off\")\n",
    "            out = os.path.join(args.out_dir, f\"recon_{i}_{b}.png\")\n",
    "            fig.tight_layout()\n",
    "            fig.savefig(out, dpi=150)\n",
    "            plt.close(fig)\n",
    "            print(f\"[{_now()}] Wrote {out}\")\n",
    "\n",
    "\n",
    "def build_parser() -> argparse.ArgumentParser:\n",
    "    p = argparse.ArgumentParser(\"\")\n",
    "    sp = p.add_subparsers(dest=\"cmd\", required=True)\n",
    "\n",
    "    p_pre = sp.add_parser(\"pretrain\", help=\"MAE self-supervised pretraining\")\n",
    "    p_pre.add_argument(\"--train-dir\", required=True)\n",
    "    p_pre.add_argument(\"--val-dir\")\n",
    "    p_pre.add_argument(\"--out-dir\", default=\"outputs/pretrain\")\n",
    "    p_pre.add_argument(\"--backbone\", choices=[\"vit_base\", \"vit_large\"], default=\"vit_large\")\n",
    "    p_pre.add_argument(\"--img-size\", type=int, default=224)\n",
    "    p_pre.add_argument(\"--patch-size\", type=int, default=16)\n",
    "    p_pre.add_argument(\"--mask-ratio\", type=float, default=0.75)\n",
    "    p_pre.add_argument(\"--norm-pix-loss\", action=\"store_true\")\n",
    "    p_pre.add_argument(\"--batch-size\", type=int, default=64)\n",
    "    p_pre.add_argument(\"--epochs\", type=int, default=50)\n",
    "    p_pre.add_argument(\"--lr\", type=float, default=1.5e-4)\n",
    "    p_pre.add_argument(\"--weight-decay\", type=float, default=0.05)\n",
    "    p_pre.add_argument(\"--num-workers\", type=int, default=4)\n",
    "    p_pre.add_argument(\"--precision\", default=\"16-mixed\")\n",
    "    p_pre.set_defaults(func=cmd_pretrain)\n",
    "\n",
    "    p_ft = sp.add_parser(\"finetune\", help=\"Fine-tune / linear-probe on labeled rare-disease data\")\n",
    "    p_ft.add_argument(\"--train-dir\", required=True)\n",
    "    p_ft.add_argument(\"--val-dir\", required=True)\n",
    "    p_ft.add_argument(\"--out-dir\", default=\"outputs/finetune\")\n",
    "    p_ft.add_argument(\"--backbone\", choices=[\"vit_base\", \"vit_large\"], default=\"vit_base\")\n",
    "    p_ft.add_argument(\"--img-size\", type=int, default=224)\n",
    "    p_ft.add_argument(\"--patch-size\", type=int, default=16)\n",
    "    p_ft.add_argument(\"--batch-size\", type=int, default=32)\n",
    "    p_ft.add_argument(\"--epochs\", type=int, default=100)\n",
    "    p_ft.add_argument(\"--lr\", type=float, default=1e-4)\n",
    "    p_ft.add_argument(\"--encoder-lr\", type=float)\n",
    "    p_ft.add_argument(\"--weight-decay\", type=float, default=0.01)\n",
    "    p_ft.add_argument(\"--num-workers\", type=int, default=4)\n",
    "    p_ft.add_argument(\"--precision\", default=\"16-mixed\")\n",
    "    p_ft.add_argument(\"--init\", choices=[\"mae\", \"imagenet\"], default=\"mae\")\n",
    "    p_ft.add_argument(\"--mae-ckpt\", help=\"Path to mae_pretrained.ckpt\")\n",
    "    p_ft.add_argument(\"--freeze-encoder\", action=\"store_true\")\n",
    "    p_ft.set_defaults(func=cmd_finetune)\n",
    "\n",
    "    p_rn = sp.add_parser(\"finetune_resnet\", help=\"ResNet baseline (frozen encoder + linear head)\")\n",
    "    p_rn.add_argument(\"--train-dir\", required=True)\n",
    "    p_rn.add_argument(\"--val-dir\", required=True)\n",
    "    p_rn.add_argument(\"--out-dir\", default=\"outputs/finetune_resnet\")\n",
    "    p_rn.add_argument(\"--arch\", choices=[\"resnet18\", \"resnet50\"], default=\"resnet50\")\n",
    "    p_rn.add_argument(\"--pretrained\", action=argparse.BooleanOptionalAction, default=True)\n",
    "    p_rn.add_argument(\"--freeze-encoder\", action=\"store_true\")\n",
    "    p_rn.add_argument(\"--img-size\", type=int, default=224)\n",
    "    p_rn.add_argument(\"--batch-size\", type=int, default=32)\n",
    "    p_rn.add_argument(\"--epochs\", type=int, default=100)\n",
    "    p_rn.add_argument(\"--lr\", type=float, default=1e-4)\n",
    "    p_rn.add_argument(\"--encoder-lr\", type=float)\n",
    "    p_rn.add_argument(\"--weight-decay\", type=float, default=0.01)\n",
    "    p_rn.add_argument(\"--num-workers\", type=int, default=4)\n",
    "    p_rn.add_argument(\"--precision\", default=\"16-mixed\")\n",
    "    p_rn.set_defaults(func=cmd_finetune_resnet)\n",
    "\n",
    "    p_rec = sp.add_parser(\"reconstruct\", help=\"Save MAE reconstructions for a folder\")\n",
    "    p_rec.add_argument(\"--images-dir\", required=True)\n",
    "    p_rec.add_argument(\"--ckpt\", required=True)\n",
    "    p_rec.add_argument(\"--out-dir\", default=\"outputs/recon\")\n",
    "    p_rec.add_argument(\"--backbone\", choices=[\"vit_base\", \"vit_large\"], default=\"vit_base\")\n",
    "    p_rec.add_argument(\"--img-size\", type=int, default=224)\n",
    "    p_rec.add_argument(\"--patch-size\", type=int, default=16)\n",
    "    p_rec.add_argument(\"--mask-ratio\", type=float, default=0.75)\n",
    "    p_rec.add_argument(\"--batch-size\", type=int, default=8)\n",
    "    p_rec.add_argument(\"--num-workers\", type=int, default=2)\n",
    "    p_rec.add_argument(\"--max-batches\", type=int, default=2)\n",
    "    p_rec.add_argument(\"--max-items\", type=int, default=4)\n",
    "    p_rec.set_defaults(func=cmd_reconstruct)\n",
    "\n",
    "    p_viz = sp.add_parser(\"visualize\", help=\"Show MAE: original vs corrupted vs reconstruction for one image\")\n",
    "    p_viz.add_argument(\"--image-path\", required=True)\n",
    "    p_viz.add_argument(\"--ckpt\", required=True)\n",
    "    p_viz.add_argument(\"--out-path\")\n",
    "    p_viz.add_argument(\"--title\")\n",
    "    p_viz.add_argument(\"--backbone\", choices=[\"vit_base\", \"vit_large\"], default=\"vit_base\")\n",
    "    p_viz.add_argument(\"--img-size\", type=int, default=224)\n",
    "    p_viz.add_argument(\"--patch-size\", type=int, default=16)\n",
    "    p_viz.add_argument(\"--mask-ratio\", type=float, default=0.75)\n",
    "    p_viz.set_defaults(func=cmd_visualize)\n",
    "\n",
    "    return p\n",
    "\n",
    "\n",
    "def main(argv: Optional[List[str]] = None):\n",
    "    if argv is None:\n",
    "        argv = sys.argv[1:]\n",
    "\n",
    "    if len(argv) == 1 and argv[0].endswith(\".json\") and os.path.exists(argv[0]):\n",
    "        build_parser().print_help()\n",
    "        return\n",
    "\n",
    "    args = build_parser().parse_args(argv)\n",
    "    args.func(args)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6c62edae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-30T14:34:35.645950Z",
     "iopub.status.busy": "2025-12-30T14:34:35.645807Z",
     "iopub.status.idle": "2025-12-30T14:41:00.016133Z",
     "shell.execute_reply": "2025-12-30T14:41:00.015531Z"
    },
    "papermill": {
     "duration": 384.374545,
     "end_time": "2025-12-30T14:41:00.017793",
     "exception": false,
     "start_time": "2025-12-30T14:34:35.643248",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "[2025-12-30 14:34:58] Environment\r\n",
      "  - python: 3.12.12\r\n",
      "  - torch: 2.8.0+cu126\r\n",
      "  - cuda_available: True\r\n",
      "  - cuda_device_count: 1\r\n",
      "  - device_name: NVIDIA H100 80GB HBM3\r\n",
      "\r\n",
      "[2025-12-30 14:34:59] Finetune dataset (train)\r\n",
      "  - root: /kaggle/input/alzheimer-dataset/archive/finetune_dataset/train\r\n",
      "  - num_samples: 600\r\n",
      "  - num_classes: 4\r\n",
      "  - class_to_idx: {'Mild Dementia': 0, 'Moderate Dementia': 1, 'Non Demented': 2, 'Very mild Dementia': 3}\r\n",
      "  - sample_preview: [{'path': '/kaggle/input/alzheimer-dataset/archive/finetune_dataset/train/Mild Dementia/OAS1_0291_MR1_mpr-1_113.jpg', 'label': 0, 'class': 'Mild Dementia'}, {'path': '/kaggle/input/alzheimer-dataset/archive/finetune_dataset/train/Mild Dementia/OAS1_0373_MR1_mpr-1_123.jpg', 'label': 0, 'class': 'Mild Dementia'}, {'path': '/kaggle/input/alzheimer-dataset/archive/finetune_dataset/train/Mild Dementia/OAS1_0278_MR1_mpr-4_105.jpg', 'label': 0, 'class': 'Mild Dementia'}]\r\n",
      "\r\n",
      "[2025-12-30 14:34:59] Finetune dataset (val)\r\n",
      "  - root: /kaggle/input/alzheimer-dataset/archive/finetune_dataset/val\r\n",
      "  - num_samples: 200\r\n",
      "  - num_classes: 4\r\n",
      "  - class_to_idx: {'Mild Dementia': 0, 'Moderate Dementia': 1, 'Non Demented': 2, 'Very mild Dementia': 3}\r\n",
      "  - sample_preview: [{'path': '/kaggle/input/alzheimer-dataset/archive/finetune_dataset/val/Mild Dementia/OAS1_0316_MR1_mpr-1_107.jpg', 'label': 0, 'class': 'Mild Dementia'}, {'path': '/kaggle/input/alzheimer-dataset/archive/finetune_dataset/val/Mild Dementia/OAS1_0382_MR1_mpr-3_145.jpg', 'label': 0, 'class': 'Mild Dementia'}, {'path': '/kaggle/input/alzheimer-dataset/archive/finetune_dataset/val/Mild Dementia/OAS1_0035_MR1_mpr-1_120.jpg', 'label': 0, 'class': 'Mild Dementia'}]\r\n",
      "\r\n",
      "[2025-12-30 14:34:59] Finetune config\r\n",
      "  - init: mae\r\n",
      "  - mae_ckpt: /kaggle/input/mae-ssl-model/049-066950-0.0000.ckpt\r\n",
      "  - backbone: vit_base\r\n",
      "  - img_size: 224\r\n",
      "  - patch_size: 16\r\n",
      "  - num_classes: 4\r\n",
      "  - freeze_encoder: False\r\n",
      "  - encoder_lr: None\r\n",
      "  - batch_size: 32\r\n",
      "  - epochs: 100\r\n",
      "  - lr: 0.0001\r\n",
      "  - weight_decay: 0.01\r\n",
      "  - precision: 16-mixed\r\n",
      "  - num_workers: 4\r\n",
      "  - out_dir: outputs/finetune\r\n",
      "[2025-12-30 14:34:59] Loading encoder weights from MAE checkpoint: /kaggle/input/mae-ssl-model/049-066950-0.0000.ckpt\r\n",
      "/usr/local/lib/python3.12/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\r\n",
      "  warnings.warn(\r\n",
      "/usr/local/lib/python3.12/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\r\n",
      "  warnings.warn(\r\n",
      "Using 16bit Automatic Mixed Precision (AMP)\r\n",
      "GPU available: True (cuda), used: True\r\n",
      "TPU available: False, using: 0 TPU cores\r\n",
      "\r\n",
      "[2025-12-30 14:35:11] Starting fine-tuning...\r\n",
      "You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\r\n",
      "2025-12-30 14:35:15.478798: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\n",
      "E0000 00:00:1767105315.892409      85 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "E0000 00:00:1767105316.023073      85 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "W0000 00:00:1767105317.113682      85 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\r\n",
      "W0000 00:00:1767105317.113717      85 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\r\n",
      "W0000 00:00:1767105317.113720      85 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\r\n",
      "W0000 00:00:1767105317.113722      85 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\r\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\r\n",
      "/usr/local/lib/python3.12/dist-packages/pytorch_lightning/utilities/model_summary/model_summary.py:242: Precision 16-mixed is not supported by the model summary.  Estimated model size in MB will not be accurate. Using 32 bits instead.\r\n",
      "\r\n",
      "\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35mName     \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35mType             \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35mParams\u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35mMode \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35mFLOPs\u001b[0m\u001b[1;35m \u001b[0m\r\n",
      "\r\n",
      "\u001b[2m \u001b[0m\u001b[2m0\u001b[0m\u001b[2m \u001b[0m encoder    VisionTransformer  85.8 M  train      0 \r\n",
      "\u001b[2m \u001b[0m\u001b[2m1\u001b[0m\u001b[2m \u001b[0m head       Linear              3.1 K  train      0 \r\n",
      "\u001b[2m \u001b[0m\u001b[2m2\u001b[0m\u001b[2m \u001b[0m criterion  CrossEntropyLoss        0  train      0 \r\n",
      "\r\n",
      "\u001b[1mTrainable params\u001b[0m: 85.8 M                                                        \r\n",
      "\u001b[1mNon-trainable params\u001b[0m: 0                                                         \r\n",
      "\u001b[1mTotal params\u001b[0m: 85.8 M                                                            \r\n",
      "\u001b[1mTotal estimated model params size (MB)\u001b[0m: 343                                     \r\n",
      "\u001b[1mModules in train mode\u001b[0m: 278                                                      \r\n",
      "\u001b[1mModules in eval mode\u001b[0m: 0                                                         \r\n",
      "\u001b[1mTotal FLOPs\u001b[0m: 0                                                                  \r\n",
      "\r\n",
      "[2025-12-30 14:35:32] Epoch 0/99\r\n",
      "[2025-12-30 14:35:35] val_loss=1.325034  val_acc=0.3900\r\n",
      "\r\n",
      "[2025-12-30 14:35:40] Epoch 1/99\r\n",
      "[2025-12-30 14:35:41] train_loss=1.376772  train_acc=0.3300  val_loss=1.236235  val_acc=0.4900\r\n",
      "\r\n",
      "[2025-12-30 14:35:56] Epoch 2/99\r\n",
      "[2025-12-30 14:35:58] train_loss=1.196751  train_acc=0.5200  val_loss=1.159960  val_acc=0.5150\r\n",
      "\r\n",
      "[2025-12-30 14:36:07] Epoch 3/99\r\n",
      "[2025-12-30 14:36:08] train_loss=1.051325  train_acc=0.6100  val_loss=1.114586  val_acc=0.5350\r\n",
      "\r\n",
      "[2025-12-30 14:36:14] Epoch 4/99\r\n",
      "[2025-12-30 14:36:15] train_loss=0.940137  train_acc=0.6600  val_loss=1.077786  val_acc=0.5600\r\n",
      "\r\n",
      "[2025-12-30 14:36:31] Epoch 5/99\r\n",
      "[2025-12-30 14:36:32] train_loss=0.875021  train_acc=0.6750  val_loss=1.045193  val_acc=0.5550\r\n",
      "\r\n",
      "[2025-12-30 14:36:45] Epoch 6/99\r\n",
      "[2025-12-30 14:36:46] train_loss=0.758939  train_acc=0.7683  val_loss=1.039767  val_acc=0.5800\r\n",
      "\r\n",
      "[2025-12-30 14:36:52] Epoch 7/99\r\n",
      "[2025-12-30 14:36:53] train_loss=0.726074  train_acc=0.7550  val_loss=0.995464  val_acc=0.6050\r\n",
      "\r\n",
      "[2025-12-30 14:36:59] Epoch 8/99\r\n",
      "[2025-12-30 14:37:00] train_loss=0.664949  train_acc=0.7783  val_loss=0.973351  val_acc=0.6250\r\n",
      "\r\n",
      "[2025-12-30 14:37:15] Epoch 9/99\r\n",
      "[2025-12-30 14:37:17] train_loss=0.565860  train_acc=0.8383  val_loss=0.946659  val_acc=0.6200\r\n",
      "\r\n",
      "[2025-12-30 14:37:24] Epoch 10/99\r\n",
      "[2025-12-30 14:37:26] train_loss=0.547055  train_acc=0.8367  val_loss=0.932919  val_acc=0.6450\r\n",
      "\r\n",
      "[2025-12-30 14:37:32] Epoch 11/99\r\n",
      "[2025-12-30 14:37:33] train_loss=0.488256  train_acc=0.8483  val_loss=0.976207  val_acc=0.5950\r\n",
      "\r\n",
      "[2025-12-30 14:37:33] Epoch 12/99\r\n",
      "[2025-12-30 14:37:34] train_loss=0.412775  train_acc=0.8833  val_loss=0.908208  val_acc=0.6450\r\n",
      "\r\n",
      "[2025-12-30 14:37:40] Epoch 13/99\r\n",
      "[2025-12-30 14:37:41] train_loss=0.415715  train_acc=0.8617  val_loss=0.898168  val_acc=0.6450\r\n",
      "\r\n",
      "[2025-12-30 14:37:58] Epoch 14/99\r\n",
      "[2025-12-30 14:38:00] train_loss=0.369055  train_acc=0.8967  val_loss=0.931852  val_acc=0.6100\r\n",
      "\r\n",
      "[2025-12-30 14:38:00] Epoch 15/99\r\n",
      "[2025-12-30 14:38:01] train_loss=0.362899  train_acc=0.8917  val_loss=0.891823  val_acc=0.6200\r\n",
      "\r\n",
      "[2025-12-30 14:38:07] Epoch 16/99\r\n",
      "[2025-12-30 14:38:08] train_loss=0.337662  train_acc=0.9067  val_loss=0.870352  val_acc=0.6200\r\n",
      "\r\n",
      "[2025-12-30 14:38:14] Epoch 17/99\r\n",
      "[2025-12-30 14:38:15] train_loss=0.310250  train_acc=0.9150  val_loss=0.877799  val_acc=0.6600\r\n",
      "\r\n",
      "[2025-12-30 14:38:15] Epoch 18/99\r\n",
      "[2025-12-30 14:38:16] train_loss=0.267180  train_acc=0.9350  val_loss=0.890973  val_acc=0.6200\r\n",
      "\r\n",
      "[2025-12-30 14:38:16] Epoch 19/99\r\n",
      "[2025-12-30 14:38:18] train_loss=0.261099  train_acc=0.9250  val_loss=0.849916  val_acc=0.6550\r\n",
      "\r\n",
      "[2025-12-30 14:38:32] Epoch 20/99\r\n",
      "[2025-12-30 14:38:33] train_loss=0.310659  train_acc=0.8967  val_loss=0.878455  val_acc=0.6500\r\n",
      "\r\n",
      "[2025-12-30 14:38:33] Epoch 21/99\r\n",
      "[2025-12-30 14:38:34] train_loss=0.228369  train_acc=0.9350  val_loss=0.932527  val_acc=0.6150\r\n",
      "\r\n",
      "[2025-12-30 14:38:34] Epoch 22/99\r\n",
      "[2025-12-30 14:38:35] train_loss=0.182917  train_acc=0.9583  val_loss=0.923201  val_acc=0.6350\r\n",
      "\r\n",
      "[2025-12-30 14:38:35] Epoch 23/99\r\n",
      "[2025-12-30 14:38:37] train_loss=0.195524  train_acc=0.9433  val_loss=0.803595  val_acc=0.7050\r\n",
      "\r\n",
      "[2025-12-30 14:38:45] Epoch 24/99\r\n",
      "[2025-12-30 14:38:46] train_loss=0.193190  train_acc=0.9450  val_loss=0.882582  val_acc=0.6650\r\n",
      "\r\n",
      "[2025-12-30 14:38:46] Epoch 25/99\r\n",
      "[2025-12-30 14:38:47] train_loss=0.177435  train_acc=0.9517  val_loss=0.846687  val_acc=0.6750\r\n",
      "\r\n",
      "[2025-12-30 14:38:47] Epoch 26/99\r\n",
      "[2025-12-30 14:38:49] train_loss=0.154429  train_acc=0.9617  val_loss=0.849144  val_acc=0.6800\r\n",
      "\r\n",
      "[2025-12-30 14:38:49] Epoch 27/99\r\n",
      "[2025-12-30 14:38:50] train_loss=0.206239  train_acc=0.9317  val_loss=0.900312  val_acc=0.6600\r\n",
      "\r\n",
      "[2025-12-30 14:38:50] Epoch 28/99\r\n",
      "[2025-12-30 14:38:51] train_loss=0.177061  train_acc=0.9450  val_loss=0.859278  val_acc=0.6750\r\n",
      "\r\n",
      "[2025-12-30 14:38:51] Epoch 29/99\r\n",
      "[2025-12-30 14:38:52] train_loss=0.134589  train_acc=0.9617  val_loss=0.789766  val_acc=0.7250\r\n",
      "\r\n",
      "[2025-12-30 14:39:01] Epoch 30/99\r\n",
      "[2025-12-30 14:39:02] train_loss=0.155998  train_acc=0.9517  val_loss=0.834714  val_acc=0.6950\r\n",
      "\r\n",
      "[2025-12-30 14:39:02] Epoch 31/99\r\n",
      "[2025-12-30 14:39:03] train_loss=0.127802  train_acc=0.9600  val_loss=0.845612  val_acc=0.7000\r\n",
      "\r\n",
      "[2025-12-30 14:39:03] Epoch 32/99\r\n",
      "[2025-12-30 14:39:04] train_loss=0.105617  train_acc=0.9767  val_loss=0.849270  val_acc=0.7000\r\n",
      "\r\n",
      "[2025-12-30 14:39:04] Epoch 33/99\r\n",
      "[2025-12-30 14:39:06] train_loss=0.136296  train_acc=0.9617  val_loss=0.875136  val_acc=0.6800\r\n",
      "\r\n",
      "[2025-12-30 14:39:06] Epoch 34/99\r\n",
      "[2025-12-30 14:39:07] train_loss=0.118827  train_acc=0.9617  val_loss=1.020898  val_acc=0.6200\r\n",
      "\r\n",
      "[2025-12-30 14:39:07] Epoch 35/99\r\n",
      "[2025-12-30 14:39:08] train_loss=0.148832  train_acc=0.9433  val_loss=0.889271  val_acc=0.7000\r\n",
      "\r\n",
      "[2025-12-30 14:39:08] Epoch 36/99\r\n",
      "[2025-12-30 14:39:09] train_loss=0.150514  train_acc=0.9500  val_loss=0.853851  val_acc=0.7200\r\n",
      "\r\n",
      "[2025-12-30 14:39:09] Epoch 37/99\r\n",
      "[2025-12-30 14:39:11] train_loss=0.122438  train_acc=0.9583  val_loss=0.896562  val_acc=0.6950\r\n",
      "\r\n",
      "[2025-12-30 14:39:11] Epoch 38/99\r\n",
      "[2025-12-30 14:39:12] train_loss=0.112133  train_acc=0.9617  val_loss=0.780753  val_acc=0.7450\r\n",
      "\r\n",
      "[2025-12-30 14:39:17] Epoch 39/99\r\n",
      "[2025-12-30 14:39:18] train_loss=0.088114  train_acc=0.9733  val_loss=0.803636  val_acc=0.7500\r\n",
      "\r\n",
      "[2025-12-30 14:39:18] Epoch 40/99\r\n",
      "[2025-12-30 14:39:20] train_loss=0.112380  train_acc=0.9650  val_loss=0.824720  val_acc=0.7500\r\n",
      "\r\n",
      "[2025-12-30 14:39:20] Epoch 41/99\r\n",
      "[2025-12-30 14:39:21] train_loss=0.121674  train_acc=0.9650  val_loss=0.861446  val_acc=0.7200\r\n",
      "\r\n",
      "[2025-12-30 14:39:21] Epoch 42/99\r\n",
      "[2025-12-30 14:39:22] train_loss=0.131557  train_acc=0.9500  val_loss=0.839036  val_acc=0.7350\r\n",
      "\r\n",
      "[2025-12-30 14:39:22] Epoch 43/99\r\n",
      "[2025-12-30 14:39:23] train_loss=0.092095  train_acc=0.9717  val_loss=0.823735  val_acc=0.7300\r\n",
      "\r\n",
      "[2025-12-30 14:39:23] Epoch 44/99\r\n",
      "[2025-12-30 14:39:25] train_loss=0.105945  train_acc=0.9567  val_loss=0.866598  val_acc=0.7050\r\n",
      "\r\n",
      "[2025-12-30 14:39:25] Epoch 45/99\r\n",
      "[2025-12-30 14:39:26] train_loss=0.111162  train_acc=0.9667  val_loss=0.868209  val_acc=0.7150\r\n",
      "\r\n",
      "[2025-12-30 14:39:26] Epoch 46/99\r\n",
      "[2025-12-30 14:39:27] train_loss=0.114961  train_acc=0.9567  val_loss=0.839448  val_acc=0.7250\r\n",
      "\r\n",
      "[2025-12-30 14:39:27] Epoch 47/99\r\n",
      "[2025-12-30 14:39:28] train_loss=0.126802  train_acc=0.9567  val_loss=0.802592  val_acc=0.7400\r\n",
      "\r\n",
      "[2025-12-30 14:39:28] Epoch 48/99\r\n",
      "[2025-12-30 14:39:30] train_loss=0.097346  train_acc=0.9733  val_loss=0.762362  val_acc=0.7550\r\n",
      "\r\n",
      "[2025-12-30 14:39:35] Epoch 49/99\r\n",
      "[2025-12-30 14:39:36] train_loss=0.098496  train_acc=0.9667  val_loss=0.782363  val_acc=0.7600\r\n",
      "\r\n",
      "[2025-12-30 14:39:36] Epoch 50/99\r\n",
      "[2025-12-30 14:39:37] train_loss=0.104258  train_acc=0.9633  val_loss=0.876057  val_acc=0.7300\r\n",
      "\r\n",
      "[2025-12-30 14:39:37] Epoch 51/99\r\n",
      "[2025-12-30 14:39:38] train_loss=0.104036  train_acc=0.9617  val_loss=0.825671  val_acc=0.7700\r\n",
      "\r\n",
      "[2025-12-30 14:39:38] Epoch 52/99\r\n",
      "[2025-12-30 14:39:40] train_loss=0.084732  train_acc=0.9750  val_loss=0.785484  val_acc=0.7700\r\n",
      "\r\n",
      "[2025-12-30 14:39:40] Epoch 53/99\r\n",
      "[2025-12-30 14:39:41] train_loss=0.063218  train_acc=0.9817  val_loss=0.783385  val_acc=0.7650\r\n",
      "\r\n",
      "[2025-12-30 14:39:41] Epoch 54/99\r\n",
      "[2025-12-30 14:39:42] train_loss=0.090399  train_acc=0.9700  val_loss=0.839028  val_acc=0.7500\r\n",
      "\r\n",
      "[2025-12-30 14:39:42] Epoch 55/99\r\n",
      "[2025-12-30 14:39:43] train_loss=0.083894  train_acc=0.9683  val_loss=0.740064  val_acc=0.7750\r\n",
      "\r\n",
      "[2025-12-30 14:39:50] Epoch 56/99\r\n",
      "[2025-12-30 14:39:51] train_loss=0.074180  train_acc=0.9717  val_loss=0.823440  val_acc=0.7700\r\n",
      "\r\n",
      "[2025-12-30 14:39:51] Epoch 57/99\r\n",
      "[2025-12-30 14:39:53] train_loss=0.080281  train_acc=0.9633  val_loss=0.778537  val_acc=0.7700\r\n",
      "\r\n",
      "[2025-12-30 14:39:53] Epoch 58/99\r\n",
      "[2025-12-30 14:39:54] train_loss=0.090180  train_acc=0.9650  val_loss=0.851561  val_acc=0.7400\r\n",
      "\r\n",
      "[2025-12-30 14:39:54] Epoch 59/99\r\n",
      "[2025-12-30 14:39:55] train_loss=0.100232  train_acc=0.9717  val_loss=0.835068  val_acc=0.7500\r\n",
      "\r\n",
      "[2025-12-30 14:39:55] Epoch 60/99\r\n",
      "[2025-12-30 14:39:56] train_loss=0.093691  train_acc=0.9700  val_loss=0.923125  val_acc=0.7350\r\n",
      "\r\n",
      "[2025-12-30 14:39:56] Epoch 61/99\r\n",
      "[2025-12-30 14:39:57] train_loss=0.075677  train_acc=0.9717  val_loss=0.890863  val_acc=0.7500\r\n",
      "\r\n",
      "[2025-12-30 14:39:57] Epoch 62/99\r\n",
      "[2025-12-30 14:39:59] train_loss=0.051993  train_acc=0.9850  val_loss=0.866246  val_acc=0.7500\r\n",
      "\r\n",
      "[2025-12-30 14:39:59] Epoch 63/99\r\n",
      "[2025-12-30 14:40:00] train_loss=0.075522  train_acc=0.9750  val_loss=0.833945  val_acc=0.7450\r\n",
      "\r\n",
      "[2025-12-30 14:40:00] Epoch 64/99\r\n",
      "[2025-12-30 14:40:01] train_loss=0.053161  train_acc=0.9800  val_loss=0.885990  val_acc=0.7500\r\n",
      "\r\n",
      "[2025-12-30 14:40:01] Epoch 65/99\r\n",
      "[2025-12-30 14:40:02] train_loss=0.075104  train_acc=0.9683  val_loss=0.920771  val_acc=0.7350\r\n",
      "\r\n",
      "[2025-12-30 14:40:02] Epoch 66/99\r\n",
      "[2025-12-30 14:40:03] train_loss=0.087543  train_acc=0.9733  val_loss=0.980215  val_acc=0.7300\r\n",
      "\r\n",
      "[2025-12-30 14:40:03] Epoch 67/99\r\n",
      "[2025-12-30 14:40:04] train_loss=0.064233  train_acc=0.9767  val_loss=0.929294  val_acc=0.7500\r\n",
      "\r\n",
      "[2025-12-30 14:40:04] Epoch 68/99\r\n",
      "[2025-12-30 14:40:05] train_loss=0.081434  train_acc=0.9667  val_loss=0.909795  val_acc=0.7400\r\n",
      "\r\n",
      "[2025-12-30 14:40:05] Epoch 69/99\r\n",
      "[2025-12-30 14:40:06] train_loss=0.073384  train_acc=0.9817  val_loss=0.840833  val_acc=0.7650\r\n",
      "\r\n",
      "[2025-12-30 14:40:06] Epoch 70/99\r\n",
      "[2025-12-30 14:40:08] train_loss=0.076740  train_acc=0.9817  val_loss=0.818022  val_acc=0.7600\r\n",
      "\r\n",
      "[2025-12-30 14:40:08] Epoch 71/99\r\n",
      "[2025-12-30 14:40:09] train_loss=0.054264  train_acc=0.9850  val_loss=0.820076  val_acc=0.7700\r\n",
      "\r\n",
      "[2025-12-30 14:40:09] Epoch 72/99\r\n",
      "[2025-12-30 14:40:10] train_loss=0.057357  train_acc=0.9850  val_loss=0.874914  val_acc=0.7400\r\n",
      "\r\n",
      "[2025-12-30 14:40:10] Epoch 73/99\r\n",
      "[2025-12-30 14:40:11] train_loss=0.085313  train_acc=0.9683  val_loss=0.996413  val_acc=0.6950\r\n",
      "\r\n",
      "[2025-12-30 14:40:11] Epoch 74/99\r\n",
      "[2025-12-30 14:40:12] train_loss=0.046898  train_acc=0.9817  val_loss=0.986724  val_acc=0.7100\r\n",
      "\r\n",
      "[2025-12-30 14:40:12] Epoch 75/99\r\n",
      "[2025-12-30 14:40:13] train_loss=0.054623  train_acc=0.9800  val_loss=1.038791  val_acc=0.6900\r\n",
      "\r\n",
      "[2025-12-30 14:40:13] Epoch 76/99\r\n",
      "[2025-12-30 14:40:14] train_loss=0.068943  train_acc=0.9750  val_loss=0.890853  val_acc=0.7500\r\n",
      "\r\n",
      "[2025-12-30 14:40:14] Epoch 77/99\r\n",
      "[2025-12-30 14:40:15] train_loss=0.078418  train_acc=0.9750  val_loss=0.816994  val_acc=0.7550\r\n",
      "\r\n",
      "[2025-12-30 14:40:15] Epoch 78/99\r\n",
      "[2025-12-30 14:40:16] train_loss=0.059317  train_acc=0.9817  val_loss=0.752706  val_acc=0.7800\r\n",
      "\r\n",
      "[2025-12-30 14:40:16] Epoch 79/99\r\n",
      "[2025-12-30 14:40:17] train_loss=0.083813  train_acc=0.9683  val_loss=0.782337  val_acc=0.7700\r\n",
      "\r\n",
      "[2025-12-30 14:40:17] Epoch 80/99\r\n",
      "[2025-12-30 14:40:18] train_loss=0.047940  train_acc=0.9833  val_loss=0.861818  val_acc=0.7350\r\n",
      "\r\n",
      "[2025-12-30 14:40:18] Epoch 81/99\r\n",
      "[2025-12-30 14:40:20] train_loss=0.060004  train_acc=0.9850  val_loss=0.760719  val_acc=0.7600\r\n",
      "\r\n",
      "[2025-12-30 14:40:20] Epoch 82/99\r\n",
      "[2025-12-30 14:40:21] train_loss=0.055950  train_acc=0.9850  val_loss=0.846758  val_acc=0.7550\r\n",
      "\r\n",
      "[2025-12-30 14:40:21] Epoch 83/99\r\n",
      "[2025-12-30 14:40:22] train_loss=0.049677  train_acc=0.9817  val_loss=0.827469  val_acc=0.7550\r\n",
      "\r\n",
      "[2025-12-30 14:40:22] Epoch 84/99\r\n",
      "[2025-12-30 14:40:23] train_loss=0.079356  train_acc=0.9667  val_loss=0.812630  val_acc=0.7450\r\n",
      "\r\n",
      "[2025-12-30 14:40:23] Epoch 85/99\r\n",
      "[2025-12-30 14:40:24] train_loss=0.057532  train_acc=0.9833  val_loss=0.902911  val_acc=0.7150\r\n",
      "\r\n",
      "[2025-12-30 14:40:24] Epoch 86/99\r\n",
      "[2025-12-30 14:40:25] train_loss=0.084656  train_acc=0.9750  val_loss=0.746103  val_acc=0.7550\r\n",
      "\r\n",
      "[2025-12-30 14:40:25] Epoch 87/99\r\n",
      "[2025-12-30 14:40:26] train_loss=0.057808  train_acc=0.9767  val_loss=0.778395  val_acc=0.7600\r\n",
      "\r\n",
      "[2025-12-30 14:40:26] Epoch 88/99\r\n",
      "[2025-12-30 14:40:27] train_loss=0.075803  train_acc=0.9683  val_loss=0.807877  val_acc=0.7550\r\n",
      "\r\n",
      "[2025-12-30 14:40:27] Epoch 89/99\r\n",
      "[2025-12-30 14:40:28] train_loss=0.069044  train_acc=0.9733  val_loss=0.727979  val_acc=0.7650\r\n",
      "\r\n",
      "[2025-12-30 14:40:34] Epoch 90/99\r\n",
      "[2025-12-30 14:40:35] train_loss=0.047626  train_acc=0.9833  val_loss=0.837719  val_acc=0.7400\r\n",
      "\r\n",
      "[2025-12-30 14:40:35] Epoch 91/99\r\n",
      "[2025-12-30 14:40:36] train_loss=0.080272  train_acc=0.9683  val_loss=0.799978  val_acc=0.7500\r\n",
      "\r\n",
      "[2025-12-30 14:40:36] Epoch 92/99\r\n",
      "[2025-12-30 14:40:37] train_loss=0.099210  train_acc=0.9700  val_loss=0.823378  val_acc=0.7400\r\n",
      "\r\n",
      "[2025-12-30 14:40:37] Epoch 93/99\r\n",
      "[2025-12-30 14:40:38] train_loss=0.084705  train_acc=0.9700  val_loss=0.820936  val_acc=0.7450\r\n",
      "\r\n",
      "[2025-12-30 14:40:38] Epoch 94/99\r\n",
      "[2025-12-30 14:40:39] train_loss=0.040627  train_acc=0.9817  val_loss=0.771466  val_acc=0.7500\r\n",
      "\r\n",
      "[2025-12-30 14:40:39] Epoch 95/99\r\n",
      "[2025-12-30 14:40:40] train_loss=0.043727  train_acc=0.9867  val_loss=0.830305  val_acc=0.7450\r\n",
      "\r\n",
      "[2025-12-30 14:40:40] Epoch 96/99\r\n",
      "[2025-12-30 14:40:41] train_loss=0.074035  train_acc=0.9733  val_loss=0.744453  val_acc=0.7600\r\n",
      "\r\n",
      "[2025-12-30 14:40:41] Epoch 97/99\r\n",
      "[2025-12-30 14:40:42] train_loss=0.095632  train_acc=0.9700  val_loss=0.691117  val_acc=0.7800\r\n",
      "\r\n",
      "[2025-12-30 14:40:50] Epoch 98/99\r\n",
      "[2025-12-30 14:40:51] train_loss=0.060482  train_acc=0.9783  val_loss=0.767743  val_acc=0.7750\r\n",
      "\r\n",
      "[2025-12-30 14:40:51] Epoch 99/99\r\n",
      "[2025-12-30 14:40:52] train_loss=0.050552  train_acc=0.9850  val_loss=0.826355  val_acc=0.7600\r\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\r\n",
      "[2025-12-30 14:40:53] Saved metrics CSV: outputs/finetune/metrics.csv\r\n",
      "[2025-12-30 14:40:53] Saved loss plot: outputs/finetune/loss_curve.png\r\n",
      "[2025-12-30 14:40:53] Saved accuracy plot: outputs/finetune/acc_curve.png\r\n",
      "`weights_only` was not set, defaulting to `False`.\r\n",
      "[2025-12-30 14:40:55] Saved checkpoint: outputs/finetune/finetuned.ckpt\r\n",
      "[2025-12-30 14:40:55] Lightning checkpoints: outputs/finetune/checkpoints\r\n"
     ]
    }
   ],
   "source": [
    "!python main.py finetune --train-dir \"/kaggle/input/alzheimer-dataset/archive/finetune_dataset/train\" --val-dir \"/kaggle/input/alzheimer-dataset/archive/finetune_dataset/val\" --mae-ckpt \"/kaggle/input/mae-ssl-model/049-066950-0.0000.ckpt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "33bffe27",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-30T14:41:00.032406Z",
     "iopub.status.busy": "2025-12-30T14:41:00.032220Z",
     "iopub.status.idle": "2025-12-30T14:43:51.510709Z",
     "shell.execute_reply": "2025-12-30T14:43:51.510150Z"
    },
    "papermill": {
     "duration": 171.48739,
     "end_time": "2025-12-30T14:43:51.512267",
     "exception": false,
     "start_time": "2025-12-30T14:41:00.024877",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "[2025-12-30 14:41:04] Environment\r\n",
      "  - python: 3.12.12\r\n",
      "  - torch: 2.8.0+cu126\r\n",
      "  - cuda_available: True\r\n",
      "  - cuda_device_count: 1\r\n",
      "  - device_name: NVIDIA H100 80GB HBM3\r\n",
      "\r\n",
      "[2025-12-30 14:41:04] Finetune (ResNet) dataset (train)\r\n",
      "  - root: /kaggle/input/alzheimer-dataset/archive/finetune_dataset/train\r\n",
      "  - num_samples: 600\r\n",
      "  - num_classes: 4\r\n",
      "  - class_to_idx: {'Mild Dementia': 0, 'Moderate Dementia': 1, 'Non Demented': 2, 'Very mild Dementia': 3}\r\n",
      "  - sample_preview: [{'path': '/kaggle/input/alzheimer-dataset/archive/finetune_dataset/train/Mild Dementia/OAS1_0291_MR1_mpr-1_113.jpg', 'label': 0, 'class': 'Mild Dementia'}, {'path': '/kaggle/input/alzheimer-dataset/archive/finetune_dataset/train/Mild Dementia/OAS1_0373_MR1_mpr-1_123.jpg', 'label': 0, 'class': 'Mild Dementia'}, {'path': '/kaggle/input/alzheimer-dataset/archive/finetune_dataset/train/Mild Dementia/OAS1_0278_MR1_mpr-4_105.jpg', 'label': 0, 'class': 'Mild Dementia'}]\r\n",
      "\r\n",
      "[2025-12-30 14:41:04] Finetune (ResNet) dataset (val)\r\n",
      "  - root: /kaggle/input/alzheimer-dataset/archive/finetune_dataset/val\r\n",
      "  - num_samples: 200\r\n",
      "  - num_classes: 4\r\n",
      "  - class_to_idx: {'Mild Dementia': 0, 'Moderate Dementia': 1, 'Non Demented': 2, 'Very mild Dementia': 3}\r\n",
      "  - sample_preview: [{'path': '/kaggle/input/alzheimer-dataset/archive/finetune_dataset/val/Mild Dementia/OAS1_0316_MR1_mpr-1_107.jpg', 'label': 0, 'class': 'Mild Dementia'}, {'path': '/kaggle/input/alzheimer-dataset/archive/finetune_dataset/val/Mild Dementia/OAS1_0382_MR1_mpr-3_145.jpg', 'label': 0, 'class': 'Mild Dementia'}, {'path': '/kaggle/input/alzheimer-dataset/archive/finetune_dataset/val/Mild Dementia/OAS1_0035_MR1_mpr-1_120.jpg', 'label': 0, 'class': 'Mild Dementia'}]\r\n",
      "\r\n",
      "[2025-12-30 14:41:04] Finetune (ResNet) config\r\n",
      "  - arch: resnet50\r\n",
      "  - pretrained: True\r\n",
      "  - freeze_encoder: False\r\n",
      "  - encoder_lr: None\r\n",
      "  - img_size: 224\r\n",
      "  - num_classes: 4\r\n",
      "  - batch_size: 32\r\n",
      "  - epochs: 100\r\n",
      "  - lr: 0.0001\r\n",
      "  - weight_decay: 0.01\r\n",
      "  - precision: 16-mixed\r\n",
      "  - num_workers: 4\r\n",
      "  - out_dir: outputs/finetune_resnet\r\n",
      "Downloading: \"https://download.pytorch.org/models/resnet50-11ad3fa6.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-11ad3fa6.pth\r\n",
      "100%|| 97.8M/97.8M [00:00<00:00, 205MB/s]\r\n",
      "Using 16bit Automatic Mixed Precision (AMP)\r\n",
      "GPU available: True (cuda), used: True\r\n",
      "TPU available: False, using: 0 TPU cores\r\n",
      "\r\n",
      "[2025-12-30 14:41:05] Starting ResNet linear-probe...\r\n",
      "You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\r\n",
      "2025-12-30 14:41:05.572974: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\n",
      "E0000 00:00:1767105665.584573     207 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "E0000 00:00:1767105665.588752     207 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "W0000 00:00:1767105665.599763     207 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\r\n",
      "W0000 00:00:1767105665.599787     207 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\r\n",
      "W0000 00:00:1767105665.599790     207 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\r\n",
      "W0000 00:00:1767105665.599792     207 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\r\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\r\n",
      "/usr/local/lib/python3.12/dist-packages/pytorch_lightning/utilities/model_summary/model_summary.py:242: Precision 16-mixed is not supported by the model summary.  Estimated model size in MB will not be accurate. Using 32 bits instead.\r\n",
      "\r\n",
      "\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35mName     \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35mType            \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35mParams\u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35mMode \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35mFLOPs\u001b[0m\u001b[1;35m \u001b[0m\r\n",
      "\r\n",
      "\u001b[2m \u001b[0m\u001b[2m0\u001b[0m\u001b[2m \u001b[0m encoder    ResNet            23.5 M  train      0 \r\n",
      "\u001b[2m \u001b[0m\u001b[2m1\u001b[0m\u001b[2m \u001b[0m head       Linear             8.2 K  train      0 \r\n",
      "\u001b[2m \u001b[0m\u001b[2m2\u001b[0m\u001b[2m \u001b[0m criterion  CrossEntropyLoss       0  train      0 \r\n",
      "\r\n",
      "\u001b[1mTrainable params\u001b[0m: 23.5 M                                                        \r\n",
      "\u001b[1mNon-trainable params\u001b[0m: 0                                                         \r\n",
      "\u001b[1mTotal params\u001b[0m: 23.5 M                                                            \r\n",
      "\u001b[1mTotal estimated model params size (MB)\u001b[0m: 94                                      \r\n",
      "\u001b[1mModules in train mode\u001b[0m: 153                                                      \r\n",
      "\u001b[1mModules in eval mode\u001b[0m: 0                                                         \r\n",
      "\u001b[1mTotal FLOPs\u001b[0m: 0                                                                  \r\n",
      "\r\n",
      "[2025-12-30 14:41:08] Epoch 0/99\r\n",
      "[2025-12-30 14:41:10] val_loss=1.395684  val_acc=0.2500\r\n",
      "\r\n",
      "[2025-12-30 14:41:11] Epoch 1/99\r\n",
      "[2025-12-30 14:41:12] train_loss=1.386468  train_acc=0.2867  val_loss=1.382700  val_acc=0.2500\r\n",
      "\r\n",
      "[2025-12-30 14:41:21] Epoch 2/99\r\n",
      "[2025-12-30 14:41:22] train_loss=1.353704  train_acc=0.4033  val_loss=1.372102  val_acc=0.2750\r\n",
      "\r\n",
      "[2025-12-30 14:41:23] Epoch 3/99\r\n",
      "[2025-12-30 14:41:24] train_loss=1.326634  train_acc=0.4500  val_loss=1.347234  val_acc=0.3500\r\n",
      "\r\n",
      "[2025-12-30 14:41:27] Epoch 4/99\r\n",
      "[2025-12-30 14:41:28] train_loss=1.292416  train_acc=0.4950  val_loss=1.290039  val_acc=0.4350\r\n",
      "\r\n",
      "[2025-12-30 14:41:29] Epoch 5/99\r\n",
      "[2025-12-30 14:41:30] train_loss=1.250965  train_acc=0.5133  val_loss=1.242920  val_acc=0.4550\r\n",
      "\r\n",
      "[2025-12-30 14:41:32] Epoch 6/99\r\n",
      "[2025-12-30 14:41:33] train_loss=1.199062  train_acc=0.5767  val_loss=1.208439  val_acc=0.4850\r\n",
      "\r\n",
      "[2025-12-30 14:41:35] Epoch 7/99\r\n",
      "[2025-12-30 14:41:36] train_loss=1.145718  train_acc=0.5867  val_loss=1.165593  val_acc=0.5100\r\n",
      "\r\n",
      "[2025-12-30 14:41:40] Epoch 8/99\r\n",
      "[2025-12-30 14:41:41] train_loss=1.085680  train_acc=0.6133  val_loss=1.135439  val_acc=0.4950\r\n",
      "\r\n",
      "[2025-12-30 14:41:43] Epoch 9/99\r\n",
      "[2025-12-30 14:41:44] train_loss=1.023078  train_acc=0.6367  val_loss=1.089908  val_acc=0.5000\r\n",
      "\r\n",
      "[2025-12-30 14:41:45] Epoch 10/99\r\n",
      "[2025-12-30 14:41:46] train_loss=0.934638  train_acc=0.6800  val_loss=1.062414  val_acc=0.5400\r\n",
      "\r\n",
      "[2025-12-30 14:41:49] Epoch 11/99\r\n",
      "[2025-12-30 14:41:50] train_loss=0.870002  train_acc=0.7300  val_loss=1.035615  val_acc=0.5600\r\n",
      "\r\n",
      "[2025-12-30 14:41:52] Epoch 12/99\r\n",
      "[2025-12-30 14:41:53] train_loss=0.807233  train_acc=0.7400  val_loss=0.987373  val_acc=0.5850\r\n",
      "\r\n",
      "[2025-12-30 14:41:58] Epoch 13/99\r\n",
      "[2025-12-30 14:41:59] train_loss=0.722392  train_acc=0.7717  val_loss=0.958968  val_acc=0.6000\r\n",
      "\r\n",
      "[2025-12-30 14:42:00] Epoch 14/99\r\n",
      "[2025-12-30 14:42:01] train_loss=0.670449  train_acc=0.7733  val_loss=0.940509  val_acc=0.6150\r\n",
      "\r\n",
      "[2025-12-30 14:42:03] Epoch 15/99\r\n",
      "[2025-12-30 14:42:04] train_loss=0.606459  train_acc=0.8283  val_loss=0.942538  val_acc=0.6100\r\n",
      "\r\n",
      "[2025-12-30 14:42:04] Epoch 16/99\r\n",
      "[2025-12-30 14:42:05] train_loss=0.521034  train_acc=0.8683  val_loss=0.950663  val_acc=0.6000\r\n",
      "\r\n",
      "[2025-12-30 14:42:05] Epoch 17/99\r\n",
      "[2025-12-30 14:42:06] train_loss=0.501859  train_acc=0.8550  val_loss=0.930456  val_acc=0.5900\r\n",
      "\r\n",
      "[2025-12-30 14:42:09] Epoch 18/99\r\n",
      "[2025-12-30 14:42:10] train_loss=0.425062  train_acc=0.8917  val_loss=0.900710  val_acc=0.6250\r\n",
      "\r\n",
      "[2025-12-30 14:42:11] Epoch 19/99\r\n",
      "[2025-12-30 14:42:12] train_loss=0.383226  train_acc=0.9050  val_loss=0.896213  val_acc=0.6300\r\n",
      "\r\n",
      "[2025-12-30 14:42:14] Epoch 20/99\r\n",
      "[2025-12-30 14:42:15] train_loss=0.336660  train_acc=0.9217  val_loss=0.881307  val_acc=0.6050\r\n",
      "\r\n",
      "[2025-12-30 14:42:20] Epoch 21/99\r\n",
      "[2025-12-30 14:42:22] train_loss=0.304506  train_acc=0.9267  val_loss=0.899737  val_acc=0.6200\r\n",
      "\r\n",
      "[2025-12-30 14:42:22] Epoch 22/99\r\n",
      "[2025-12-30 14:42:23] train_loss=0.283709  train_acc=0.9350  val_loss=0.890522  val_acc=0.6000\r\n",
      "\r\n",
      "[2025-12-30 14:42:23] Epoch 23/99\r\n",
      "[2025-12-30 14:42:24] train_loss=0.262665  train_acc=0.9400  val_loss=0.917125  val_acc=0.6100\r\n",
      "\r\n",
      "[2025-12-30 14:42:24] Epoch 24/99\r\n",
      "[2025-12-30 14:42:25] train_loss=0.215674  train_acc=0.9633  val_loss=0.942365  val_acc=0.5900\r\n",
      "\r\n",
      "[2025-12-30 14:42:25] Epoch 25/99\r\n",
      "[2025-12-30 14:42:26] train_loss=0.199634  train_acc=0.9517  val_loss=0.898415  val_acc=0.6050\r\n",
      "\r\n",
      "[2025-12-30 14:42:26] Epoch 26/99\r\n",
      "[2025-12-30 14:42:27] train_loss=0.175139  train_acc=0.9650  val_loss=0.910990  val_acc=0.5800\r\n",
      "\r\n",
      "[2025-12-30 14:42:27] Epoch 27/99\r\n",
      "[2025-12-30 14:42:29] train_loss=0.144025  train_acc=0.9750  val_loss=0.907558  val_acc=0.6100\r\n",
      "\r\n",
      "[2025-12-30 14:42:29] Epoch 28/99\r\n",
      "[2025-12-30 14:42:30] train_loss=0.169129  train_acc=0.9517  val_loss=0.898836  val_acc=0.6100\r\n",
      "\r\n",
      "[2025-12-30 14:42:30] Epoch 29/99\r\n",
      "[2025-12-30 14:42:31] train_loss=0.129823  train_acc=0.9767  val_loss=0.927605  val_acc=0.6200\r\n",
      "\r\n",
      "[2025-12-30 14:42:31] Epoch 30/99\r\n",
      "[2025-12-30 14:42:32] train_loss=0.122350  train_acc=0.9683  val_loss=0.936710  val_acc=0.6100\r\n",
      "\r\n",
      "[2025-12-30 14:42:32] Epoch 31/99\r\n",
      "[2025-12-30 14:42:33] train_loss=0.109711  train_acc=0.9750  val_loss=0.947148  val_acc=0.6100\r\n",
      "\r\n",
      "[2025-12-30 14:42:33] Epoch 32/99\r\n",
      "[2025-12-30 14:42:34] train_loss=0.146243  train_acc=0.9567  val_loss=0.975632  val_acc=0.6150\r\n",
      "\r\n",
      "[2025-12-30 14:42:34] Epoch 33/99\r\n",
      "[2025-12-30 14:42:35] train_loss=0.113772  train_acc=0.9683  val_loss=0.918245  val_acc=0.6300\r\n",
      "\r\n",
      "[2025-12-30 14:42:35] Epoch 34/99\r\n",
      "[2025-12-30 14:42:36] train_loss=0.111690  train_acc=0.9750  val_loss=0.968792  val_acc=0.6100\r\n",
      "\r\n",
      "[2025-12-30 14:42:36] Epoch 35/99\r\n",
      "[2025-12-30 14:42:38] train_loss=0.103584  train_acc=0.9767  val_loss=0.942457  val_acc=0.6100\r\n",
      "\r\n",
      "[2025-12-30 14:42:38] Epoch 36/99\r\n",
      "[2025-12-30 14:42:39] train_loss=0.086285  train_acc=0.9833  val_loss=0.955218  val_acc=0.6200\r\n",
      "\r\n",
      "[2025-12-30 14:42:39] Epoch 37/99\r\n",
      "[2025-12-30 14:42:40] train_loss=0.069978  train_acc=0.9867  val_loss=0.967468  val_acc=0.6200\r\n",
      "\r\n",
      "[2025-12-30 14:42:40] Epoch 38/99\r\n",
      "[2025-12-30 14:42:41] train_loss=0.096015  train_acc=0.9767  val_loss=0.959582  val_acc=0.6300\r\n",
      "\r\n",
      "[2025-12-30 14:42:41] Epoch 39/99\r\n",
      "[2025-12-30 14:42:42] train_loss=0.070743  train_acc=0.9850  val_loss=0.927083  val_acc=0.6400\r\n",
      "\r\n",
      "[2025-12-30 14:42:42] Epoch 40/99\r\n",
      "[2025-12-30 14:42:43] train_loss=0.071095  train_acc=0.9767  val_loss=0.970532  val_acc=0.6350\r\n",
      "\r\n",
      "[2025-12-30 14:42:43] Epoch 41/99\r\n",
      "[2025-12-30 14:42:44] train_loss=0.066036  train_acc=0.9850  val_loss=1.008052  val_acc=0.6400\r\n",
      "\r\n",
      "[2025-12-30 14:42:44] Epoch 42/99\r\n",
      "[2025-12-30 14:42:46] train_loss=0.080881  train_acc=0.9717  val_loss=0.924021  val_acc=0.6400\r\n",
      "\r\n",
      "[2025-12-30 14:42:46] Epoch 43/99\r\n",
      "[2025-12-30 14:42:47] train_loss=0.093636  train_acc=0.9667  val_loss=0.988700  val_acc=0.6250\r\n",
      "\r\n",
      "[2025-12-30 14:42:47] Epoch 44/99\r\n",
      "[2025-12-30 14:42:48] train_loss=0.053580  train_acc=0.9850  val_loss=0.973673  val_acc=0.6550\r\n",
      "\r\n",
      "[2025-12-30 14:42:48] Epoch 45/99\r\n",
      "[2025-12-30 14:42:49] train_loss=0.108218  train_acc=0.9683  val_loss=1.029263  val_acc=0.6400\r\n",
      "\r\n",
      "[2025-12-30 14:42:49] Epoch 46/99\r\n",
      "[2025-12-30 14:42:50] train_loss=0.071143  train_acc=0.9783  val_loss=1.015874  val_acc=0.6500\r\n",
      "\r\n",
      "[2025-12-30 14:42:50] Epoch 47/99\r\n",
      "[2025-12-30 14:42:51] train_loss=0.063704  train_acc=0.9833  val_loss=0.992955  val_acc=0.6450\r\n",
      "\r\n",
      "[2025-12-30 14:42:51] Epoch 48/99\r\n",
      "[2025-12-30 14:42:52] train_loss=0.063242  train_acc=0.9817  val_loss=0.931334  val_acc=0.6550\r\n",
      "\r\n",
      "[2025-12-30 14:42:52] Epoch 49/99\r\n",
      "[2025-12-30 14:42:53] train_loss=0.063271  train_acc=0.9850  val_loss=0.945605  val_acc=0.6550\r\n",
      "\r\n",
      "[2025-12-30 14:42:53] Epoch 50/99\r\n",
      "[2025-12-30 14:42:55] train_loss=0.057335  train_acc=0.9817  val_loss=0.976785  val_acc=0.6100\r\n",
      "\r\n",
      "[2025-12-30 14:42:55] Epoch 51/99\r\n",
      "[2025-12-30 14:42:56] train_loss=0.055923  train_acc=0.9850  val_loss=0.980377  val_acc=0.6350\r\n",
      "\r\n",
      "[2025-12-30 14:42:56] Epoch 52/99\r\n",
      "[2025-12-30 14:42:57] train_loss=0.067518  train_acc=0.9817  val_loss=0.997210  val_acc=0.6300\r\n",
      "\r\n",
      "[2025-12-30 14:42:57] Epoch 53/99\r\n",
      "[2025-12-30 14:42:58] train_loss=0.071517  train_acc=0.9767  val_loss=0.958221  val_acc=0.6250\r\n",
      "\r\n",
      "[2025-12-30 14:42:58] Epoch 54/99\r\n",
      "[2025-12-30 14:42:59] train_loss=0.052444  train_acc=0.9800  val_loss=0.951303  val_acc=0.6450\r\n",
      "\r\n",
      "[2025-12-30 14:42:59] Epoch 55/99\r\n",
      "[2025-12-30 14:43:00] train_loss=0.047207  train_acc=0.9883  val_loss=0.970324  val_acc=0.6350\r\n",
      "\r\n",
      "[2025-12-30 14:43:00] Epoch 56/99\r\n",
      "[2025-12-30 14:43:01] train_loss=0.031801  train_acc=0.9967  val_loss=0.948502  val_acc=0.6500\r\n",
      "\r\n",
      "[2025-12-30 14:43:01] Epoch 57/99\r\n",
      "[2025-12-30 14:43:02] train_loss=0.050757  train_acc=0.9850  val_loss=0.965394  val_acc=0.6450\r\n",
      "\r\n",
      "[2025-12-30 14:43:02] Epoch 58/99\r\n",
      "[2025-12-30 14:43:03] train_loss=0.038452  train_acc=0.9883  val_loss=0.959703  val_acc=0.6500\r\n",
      "\r\n",
      "[2025-12-30 14:43:03] Epoch 59/99\r\n",
      "[2025-12-30 14:43:05] train_loss=0.067520  train_acc=0.9750  val_loss=0.922723  val_acc=0.6550\r\n",
      "\r\n",
      "[2025-12-30 14:43:05] Epoch 60/99\r\n",
      "[2025-12-30 14:43:06] train_loss=0.043942  train_acc=0.9917  val_loss=0.992364  val_acc=0.6350\r\n",
      "\r\n",
      "[2025-12-30 14:43:06] Epoch 61/99\r\n",
      "[2025-12-30 14:43:07] train_loss=0.047552  train_acc=0.9833  val_loss=0.982772  val_acc=0.6450\r\n",
      "\r\n",
      "[2025-12-30 14:43:07] Epoch 62/99\r\n",
      "[2025-12-30 14:43:08] train_loss=0.037246  train_acc=0.9900  val_loss=1.004383  val_acc=0.6500\r\n",
      "\r\n",
      "[2025-12-30 14:43:08] Epoch 63/99\r\n",
      "[2025-12-30 14:43:09] train_loss=0.036362  train_acc=0.9883  val_loss=1.030880  val_acc=0.6650\r\n",
      "\r\n",
      "[2025-12-30 14:43:09] Epoch 64/99\r\n",
      "[2025-12-30 14:43:10] train_loss=0.045353  train_acc=0.9867  val_loss=0.999575  val_acc=0.6650\r\n",
      "\r\n",
      "[2025-12-30 14:43:10] Epoch 65/99\r\n",
      "[2025-12-30 14:43:11] train_loss=0.042966  train_acc=0.9833  val_loss=1.045088  val_acc=0.6550\r\n",
      "\r\n",
      "[2025-12-30 14:43:11] Epoch 66/99\r\n",
      "[2025-12-30 14:43:12] train_loss=0.032941  train_acc=0.9900  val_loss=1.039127  val_acc=0.6500\r\n",
      "\r\n",
      "[2025-12-30 14:43:12] Epoch 67/99\r\n",
      "[2025-12-30 14:43:13] train_loss=0.057589  train_acc=0.9783  val_loss=0.989674  val_acc=0.6600\r\n",
      "\r\n",
      "[2025-12-30 14:43:13] Epoch 68/99\r\n",
      "[2025-12-30 14:43:14] train_loss=0.036192  train_acc=0.9883  val_loss=1.052353  val_acc=0.6650\r\n",
      "\r\n",
      "[2025-12-30 14:43:14] Epoch 69/99\r\n",
      "[2025-12-30 14:43:15] train_loss=0.031867  train_acc=0.9900  val_loss=1.041500  val_acc=0.6500\r\n",
      "\r\n",
      "[2025-12-30 14:43:15] Epoch 70/99\r\n",
      "[2025-12-30 14:43:17] train_loss=0.040223  train_acc=0.9883  val_loss=1.013322  val_acc=0.6600\r\n",
      "\r\n",
      "[2025-12-30 14:43:17] Epoch 71/99\r\n",
      "[2025-12-30 14:43:18] train_loss=0.024869  train_acc=0.9933  val_loss=0.968730  val_acc=0.6650\r\n",
      "\r\n",
      "[2025-12-30 14:43:18] Epoch 72/99\r\n",
      "[2025-12-30 14:43:19] train_loss=0.046776  train_acc=0.9867  val_loss=0.927503  val_acc=0.6700\r\n",
      "\r\n",
      "[2025-12-30 14:43:19] Epoch 73/99\r\n",
      "[2025-12-30 14:43:20] train_loss=0.038124  train_acc=0.9867  val_loss=0.937231  val_acc=0.6550\r\n",
      "\r\n",
      "[2025-12-30 14:43:20] Epoch 74/99\r\n",
      "[2025-12-30 14:43:21] train_loss=0.028739  train_acc=0.9917  val_loss=1.002658  val_acc=0.6650\r\n",
      "\r\n",
      "[2025-12-30 14:43:21] Epoch 75/99\r\n",
      "[2025-12-30 14:43:22] train_loss=0.031086  train_acc=0.9850  val_loss=0.936442  val_acc=0.6650\r\n",
      "\r\n",
      "[2025-12-30 14:43:22] Epoch 76/99\r\n",
      "[2025-12-30 14:43:23] train_loss=0.026508  train_acc=0.9917  val_loss=1.019572  val_acc=0.6300\r\n",
      "\r\n",
      "[2025-12-30 14:43:23] Epoch 77/99\r\n",
      "[2025-12-30 14:43:24] train_loss=0.032363  train_acc=0.9933  val_loss=1.061525  val_acc=0.6400\r\n",
      "\r\n",
      "[2025-12-30 14:43:24] Epoch 78/99\r\n",
      "[2025-12-30 14:43:25] train_loss=0.043119  train_acc=0.9883  val_loss=1.047869  val_acc=0.6450\r\n",
      "\r\n",
      "[2025-12-30 14:43:25] Epoch 79/99\r\n",
      "[2025-12-30 14:43:27] train_loss=0.034275  train_acc=0.9817  val_loss=1.012147  val_acc=0.6550\r\n",
      "\r\n",
      "[2025-12-30 14:43:27] Epoch 80/99\r\n",
      "[2025-12-30 14:43:28] train_loss=0.026163  train_acc=0.9950  val_loss=0.976686  val_acc=0.6700\r\n",
      "\r\n",
      "[2025-12-30 14:43:28] Epoch 81/99\r\n",
      "[2025-12-30 14:43:29] train_loss=0.037175  train_acc=0.9933  val_loss=0.932916  val_acc=0.6650\r\n",
      "\r\n",
      "[2025-12-30 14:43:29] Epoch 82/99\r\n",
      "[2025-12-30 14:43:30] train_loss=0.033989  train_acc=0.9883  val_loss=0.941110  val_acc=0.6600\r\n",
      "\r\n",
      "[2025-12-30 14:43:30] Epoch 83/99\r\n",
      "[2025-12-30 14:43:31] train_loss=0.041137  train_acc=0.9883  val_loss=0.934062  val_acc=0.6800\r\n",
      "\r\n",
      "[2025-12-30 14:43:31] Epoch 84/99\r\n",
      "[2025-12-30 14:43:32] train_loss=0.047997  train_acc=0.9867  val_loss=0.962937  val_acc=0.6750\r\n",
      "\r\n",
      "[2025-12-30 14:43:32] Epoch 85/99\r\n",
      "[2025-12-30 14:43:33] train_loss=0.021149  train_acc=0.9933  val_loss=0.998378  val_acc=0.6750\r\n",
      "\r\n",
      "[2025-12-30 14:43:33] Epoch 86/99\r\n",
      "[2025-12-30 14:43:34] train_loss=0.031289  train_acc=0.9900  val_loss=0.956406  val_acc=0.6550\r\n",
      "\r\n",
      "[2025-12-30 14:43:34] Epoch 87/99\r\n",
      "[2025-12-30 14:43:35] train_loss=0.020425  train_acc=0.9933  val_loss=1.014868  val_acc=0.6750\r\n",
      "\r\n",
      "[2025-12-30 14:43:35] Epoch 88/99\r\n",
      "[2025-12-30 14:43:37] train_loss=0.029018  train_acc=0.9933  val_loss=1.000475  val_acc=0.6550\r\n",
      "\r\n",
      "[2025-12-30 14:43:37] Epoch 89/99\r\n",
      "[2025-12-30 14:43:38] train_loss=0.032934  train_acc=0.9900  val_loss=1.057398  val_acc=0.6550\r\n",
      "\r\n",
      "[2025-12-30 14:43:38] Epoch 90/99\r\n",
      "[2025-12-30 14:43:39] train_loss=0.028532  train_acc=0.9917  val_loss=0.982963  val_acc=0.6650\r\n",
      "\r\n",
      "[2025-12-30 14:43:39] Epoch 91/99\r\n",
      "[2025-12-30 14:43:40] train_loss=0.026921  train_acc=0.9917  val_loss=0.905056  val_acc=0.6750\r\n",
      "\r\n",
      "[2025-12-30 14:43:40] Epoch 92/99\r\n",
      "[2025-12-30 14:43:41] train_loss=0.030909  train_acc=0.9950  val_loss=0.941009  val_acc=0.6800\r\n",
      "\r\n",
      "[2025-12-30 14:43:41] Epoch 93/99\r\n",
      "[2025-12-30 14:43:42] train_loss=0.033282  train_acc=0.9900  val_loss=0.929068  val_acc=0.6750\r\n",
      "\r\n",
      "[2025-12-30 14:43:42] Epoch 94/99\r\n",
      "[2025-12-30 14:43:43] train_loss=0.018284  train_acc=0.9933  val_loss=0.923091  val_acc=0.6950\r\n",
      "\r\n",
      "[2025-12-30 14:43:43] Epoch 95/99\r\n",
      "[2025-12-30 14:43:44] train_loss=0.037740  train_acc=0.9850  val_loss=0.952037  val_acc=0.6700\r\n",
      "\r\n",
      "[2025-12-30 14:43:44] Epoch 96/99\r\n",
      "[2025-12-30 14:43:45] train_loss=0.027231  train_acc=0.9950  val_loss=0.899978  val_acc=0.6950\r\n",
      "\r\n",
      "[2025-12-30 14:43:45] Epoch 97/99\r\n",
      "[2025-12-30 14:43:46] train_loss=0.025928  train_acc=0.9933  val_loss=0.895958  val_acc=0.6900\r\n",
      "\r\n",
      "[2025-12-30 14:43:46] Epoch 98/99\r\n",
      "[2025-12-30 14:43:47] train_loss=0.025158  train_acc=0.9883  val_loss=0.948574  val_acc=0.6800\r\n",
      "\r\n",
      "[2025-12-30 14:43:47] Epoch 99/99\r\n",
      "[2025-12-30 14:43:48] train_loss=0.035233  train_acc=0.9900  val_loss=1.001879  val_acc=0.6650\r\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\r\n",
      "[2025-12-30 14:43:49] Saved metrics CSV: outputs/finetune_resnet/metrics.csv\r\n",
      "[2025-12-30 14:43:49] Saved loss plot: outputs/finetune_resnet/loss_curve.png\r\n",
      "[2025-12-30 14:43:49] Saved accuracy plot: outputs/finetune_resnet/acc_curve.png\r\n",
      "`weights_only` was not set, defaulting to `False`.\r\n",
      "[2025-12-30 14:43:49] Saved checkpoint: outputs/finetune_resnet/finetuned.ckpt\r\n",
      "[2025-12-30 14:43:49] Lightning checkpoints: outputs/finetune_resnet/checkpoints\r\n"
     ]
    }
   ],
   "source": [
    "!python main.py finetune_resnet --train-dir \"/kaggle/input/alzheimer-dataset/archive/finetune_dataset/train\" --val-dir \"/kaggle/input/alzheimer-dataset/archive/finetune_dataset/val\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "df119629",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-30T14:43:51.533078Z",
     "iopub.status.busy": "2025-12-30T14:43:51.532871Z",
     "iopub.status.idle": "2025-12-30T14:44:00.863796Z",
     "shell.execute_reply": "2025-12-30T14:44:00.863280Z"
    },
    "papermill": {
     "duration": 9.343327,
     "end_time": "2025-12-30T14:44:00.865107",
     "exception": false,
     "start_time": "2025-12-30T14:43:51.521780",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\r\n",
      "  warnings.warn(\r\n",
      "/usr/local/lib/python3.12/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\r\n",
      "  warnings.warn(\r\n",
      "Figure(1000x350)\r\n"
     ]
    }
   ],
   "source": [
    "!python main.py visualize --image-path \"/kaggle/input/alzheimer-dataset/archive/finetune_dataset/train/Mild Dementia/OAS1_0028_MR1_mpr-1_133.jpg\" --ckpt \"/kaggle/input/mae-ssl-model/049-066950-0.0000.ckpt\" --out-path \"/kaggle/working/visualization\" --title \"mae-model-reconstruction\""
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaH100",
   "dataSources": [
    {
     "databundleVersionId": 14559231,
     "isSourceIdPinned": false,
     "sourceId": 118448,
     "sourceType": "competition"
    },
    {
     "datasetId": 9150255,
     "sourceId": 14332241,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 9157919,
     "sourceId": 14342895,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 9158067,
     "sourceId": 14343102,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31234,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 568.754692,
   "end_time": "2025-12-30T14:44:01.089707",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-12-30T14:34:32.335015",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
